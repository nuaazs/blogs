# Resilient backpropagation

正常使用的反向传播算法有两个缺点，其一为学习过程中学习率的选择比较困难，一旦学习率选择不当会造成学习效果不好；其二为反向传播算法的梯度弥散作用，即距离输出层约远的神经元学习的速度越慢。

Martin Ridmiller因此提出了弹性反向传播算法`Rprop`。



## 学习率

反向传播算法中的学习率为用户提前设定的固定的$\eta$，并在整个网络中使用单一的$\eta$，因此就会出现学习率的选择问题，而在Rprop中，每一个可优化的权重都对应着一个单独的$\eta$，不同的权重的学习率不同，并且这些学习率并不是由用户指定，而是由程序自动的设定，这些学习率在执行过程中也并不是静态的而是每一个学习时间点学习率都在不断地更新，即$\eta_{i,j}(t)$。

$\Delta \omega_{i, j}=\left\{\begin{array}{c}-\eta_{i, j}(t), \text { ifg }(t)>0 \\ +\eta_{i, j}(t), \text { ifg }(t)<0 \\ 0, \text { otherwise }\end{array}\right.$



## 权重的更新

在一般的反向传播算法中，学习过程中权重的改变量是由误差函数对该权重的偏导（即梯度）所决定，而在Rprop中，权重的变化量$\Delta w_{i,j}$直接等于学习率$\eta_{i,j}(t)$，如果梯度为负数，就应该增大相应的权重来使误差函数逼近最小值，如下：

至此已经明确了权重如何更新，接下来说明一下学习率$\eta_{i,j}(t)$如何更新。此时首先应当考虑一下如何t和（t-1）两个时间点的梯度的符号会如何变化，总共有两种情况。

如果（t-1）和t两个时间点误差函数的梯度符号不同，说明在t时我们已经越过了最小值，说明上一次权值的更新步跨太大，则$\eta_{i,j}(t)$就应当比$\eta_{i,j}(t-1)$更小来使得对于最低值的搜索更加精确，在数学层面上，我们使上一步的学习率和一个大于0小于1的值$\eta^{up}(t)$相乘来得到当前的学习率。然而，当两次的符号相同，说明还未到达误差函数的最低点，可以使相应的学习率增加一些来加快学习的步伐，因此我们可以使上一步的学习率乘以一个大于1$\eta^{down}(t)$的来得到当前的学习率。
如下式所示：
$\eta_{i, j}(t)=\left\{\begin{array}{c}\eta^{u p} \eta_{i, j}(t-1), g(t-1) g(t)>0 \\ \eta^{\text {down }} \eta_{i, j}(t-1), g(t-1) g(t)<0 \\ \eta_{i, j}(t-1), \text { otherwise }\end{array}\right.$

