> 共线性，即同线性或同线型。统计学中，共线性即多重共线性。
> 多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。
> 一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。完全共线性的情况并不多见，一般出现的是在一定程度上的共线性，即近似共线性。

## 1. 共线性的一般性的影响

太多相关度很高的特征并没有提供太多的信息量，并没有提高数据可以达到的上限，相反，数据集拥有更多的特征意味着更容易收到噪声的影响，更容易收到特征偏移的影响等等，简单举个例子，N个特征全都不受到到内在或者外在因素干扰的概率为k，则2N个特征全部不受到内在或外在因素干扰的概率必然远小于k。这个问题实际上对于各类算法都存在着一定的不良影响。



## 2. 线性回归和逻辑回归是怎么受到共线性的影响

逻辑回归的梯度更新公式：

转为代码：

```python
weights = weights - alpha * dataMatrix.transpose()*error
```

其中alpha为学习率，dataMatrix.transpose()为原始数据的矩阵，error=y_pred-y_true.

**从这里可以看出，共线性问题对于逻辑回归损失函数的最优化没影响，参数都是一样更新，一样更新到收敛为止。所以对于预测来说没什么影响。**

那共线性会引发什么问题？

1、模型参数估计不准确，有时甚至会出现回归系数的符号与实际情况完全相反的情况，比如逻辑上应该系数为正的特征系数 算出来为负。

2、本应该显著的自变量不显著，本不显著的自变量却呈现出显著性（也就是说，无法从p-值的大小判断出变量是否显著——下面会给一个例子）

3、多重共线性使参数估计值的方差增大，模型参数不稳定，也就是每次训练得到的权重系数差异都比较大。

其实多重共线性这样理解会简单很多:

假设原始的线性回归公式为：

```python
y = w1*x1 + w2*x2 + w3*x3
```

训练完毕的线性回归公式为：

```python
y=5*x1+7*x2+10*x3
```

此时加入一个新特征x4，假设x4和x3高度相关，`x4=2*x3`,则

```python
y=w1*x1+w2*x2+w3*x3+w4*x4=w1*x1+w2*x2+(w3+2w4)*x3
```

因为我们之前拟合出来的最优的回归方程为：

```python
y=5*x1+7*x2+10*x3
```

显然`w3+2w4`可以合并成一个新的权重系数 `w5`，则

```python
y=w1*x1+w2*x2+w5*x3
```

显然：

`y=w1*x1+w2*x2+w3*x3`和`y=w1*x1+w2*x2+w5*x3`是等价的

那么最终最优的模型应该也是 `y=5x1+7x2+10x3`

但是考虑到引入了x4，所以w4和w3的权重是分开计算出来的，这就导致了`w5=10=w3+2w4`，显然这个方程有无穷多的解，比如w3=4，w4=3，或者w4=-1，w3=12等，因此导致了模型系数估计的不稳定并且可能会出现负系数的问题。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/files/v2-6902bb65ed48752b39a8fdce9c7ffbc9_r.jpg)

## **先从线性回归说起。**

关于statsmodel，这里也介绍了每个指标的含义：

https://blog.csdn.net/zm147451753/article/details/83107535