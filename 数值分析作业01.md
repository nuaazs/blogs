## 20.（上机题）Newton迭代法

(1) 给定初值 $x$ 及容许误差,编制 *Newton* 法解方程 $f(x)=0$ 根的通用程序。

(2) 给定方程$f(x)=x3/3-x=0$,易知其有三个根,$$ x_{1}^{*}=-\sqrt{3} $$ , $$ x_{2}^{*}=0 $$ , $$ x_{2}^{*}=\sqrt{3} $$ 

1. 由 *Newton*方法的局部收敛性可知存在$\delta>0$,当$x∈(-\delta,\delta)$时 *Newton*迭代序列收敛于根$$ x_{2}^{*}$$ ,试确定尽可能大的$\delta$;

2. 试取若干初始值,观察当$$ x_{0} \in(-\infty,-1),(-1,-\delta),(-\delta, \delta),(\delta, 1),(1,\infty)$$ 时 *Newton*序列是否收敛以及收敛于哪一个根。

(3) 通过本上机题,你明白了什么?



#### 题解：

#### (1.1). 完整代码

```python
#!/user/bin/env python  
#2021-03-20  
import numpy as np  
import time  
import math  
  
#  User Parameters  
initial = 0.8                           ## ①
epsilon = 1e-5                          ## ②
max_itr = 50                            ## ③
delta = 1e-5
# function                              ## ④
def func(x):   
    return  x**3/3. - x  
  
# Get the derivative  
def derivative(x0,delta):               ## ⑤
    return (func(x0+delta)-func(x0))/delta  
  
for itr in range(max_itr):   
    previous = initial  
    value = func(initial)  
    k = derivative(previous,1e-5)  
    initial -= (value / k)              ## ⑥
    if (abs(func(initial) - 0) < epsilon) :  
        break  
     
print("Result: x={}\n".format(initial))  
```



#### (1.2). 代码解读

①迭代初始值。

②容许误差：当迭代结果满足容许误差时退出迭代。

③允许最大迭代次数：当迭代次数达到该值时，停止迭代，返回结果。

④待求解的方程。

⑤利用导数的定义，$$\Delta y / \Delta x$$ 来获取方程某一点处的导数值。

⑥$$ x_{n+1}=x_{n}-\frac{f\left(x_{n}\right)}{f^{\prime}\left(x_{n}\right)} $$ 迭代求解， 最终结果保存在$$initial$$变量中。



#### (1.3). 牛顿迭代法和梯度下降法的比较以及可视化代码

```python
import numpy as np
import matplotlib.pyplot as plt
import mpl_toolkits.axisartist as axisartist
import time

#   User Parameters
initial = -2
initial_gd = 0.5
epsilon = 1e-10
step = 0.3
x_range = np.array([-3, 3])
plot = True


def func(x):
    return x**3/3. - x


def func_gd(x):
    return x**2. - 1


# Get Gradient
def derivative(x0, delta, func=func):
    return (func(x0+delta)-func(x0))/delta


# Main
if __name__ == "__main__":
    if(plot):
        fig = plt.figure(figsize=(8, 8))
        ax = axisartist.Subplot(fig, 111)
        fig.add_axes(ax)
        ax.axis[:].set_visible(False)

        ax.axis["x"] = ax.new_floating_axis(0, 0)
        ax.axis["x"].set_axisline_style("->", size=1.0)
        ax.axis["y"] = ax.new_floating_axis(1, 0)
        ax.axis["y"].set_axisline_style("-|>", size=1.0)
        ax.axis["x"].set_axis_direction("top")
        ax.axis["y"].set_axis_direction("right")
        x_data = np.linspace(start=-3, stop=3, num=50)
        y_data = func(x_data)
        ax.plot(x_data, y_data)
        previous_gd = initial
        k = derivative(initial, 1e-5)
        lines_gd = ax.plot(x_range, k*(x_range-initial), linewidth=2,
                           linestyle='solid', color='blue', label=r'Gradient Descent')
        lines = ax.plot(x_range, k*(x_range-initial), linewidth=2,
                        linestyle='solid', color='red', label=r"Newton's method")
        plt.ion()
        plt.legend()
        plt.show()
    #t1 = time.time()
    for rep in range(1):
        initial = 0.5
        initial_gd = 0.5
        gd_ok = False
        nt_ok = False

        ######################  M  A  I  N  ############################
        for itr in range(0, 1000):
            if(nt_ok != True):
                previous = initial
                value = func(initial)
                k = derivative(previous, 1e-5)
                initial -= (value / k)
                if(plot):
                    try:
                        ax.lines.remove(lines[0])
                        dot.remove()
                        text.remove()
                    except Exception:
                        pass
                    dot = ax.scatter(previous, value, marker="o", color='red')
                    lines = ax.plot(x_range, k*(x_range-initial), linewidth=2,
                                    linestyle='solid', color='red', label=r"Newton's method")
                    #text= plt.text(0.5, 6, "Line:\ny={:+.10f}*\n(x-{:.10f})".format(float(k),float(x00)), size = 15, alpha = 0.2,color="black")
                    plt.pause(2)

                if abs(value - 0) < epsilon:
                    itr_n = itr
                    #t_n = time.time()-t1
                    abs_n = abs(value - 0)
                    output_nt = initial
                    nt_ok = True
                    if(plot):
                        text_done = plt.text(-4, -2, "Newton's method : Done.",
                                             size=15, alpha=0.2, color="Red")
                        plt.pause(2)
                        print("nt okay!")
                    # break

            if(gd_ok != True):
                previous_gd = initial_gd
                value_gd = func_gd(initial_gd)
                k_gd = derivative(previous_gd, 1e-5, func=func_gd)
                initial_gd = previous_gd - step*k_gd
                if(plot):
                    try:
                        ax.lines.remove(lines_gd[0])
                        dot_gd.remove()

                    except Exception:
                        pass
                    dot_gd = ax.scatter(previous_gd, func(
                        previous_gd), marker="v", color="blue")
                    lines_gd = ax.plot(x_range, derivative(previous_gd, 1e-5)*(x_range-previous_gd)+func(
                        previous_gd), linewidth=2, linestyle='dashed', color='blue', label=r'Gradient Descent')
                    plt.pause(2)

                # 如果小于精度值则退出迭代
                if abs(func_gd(initial_gd) - func_gd(0)) < epsilon:
                    itr_g = itr
                    output_gd = initial_gd
                    #t_g = time.time()-t1
                    abs_g = abs(func_gd(initial_gd) - func_gd(0))
                    gd_ok = True
                    if(plot):
                        text_done = plt.text(-5, -2, "Newton's method : Done.",
                                             size=15, alpha=0.2, color="Red")
                        plt.pause(2)
                        print("gd okay!")
            itr += 1
            if nt_ok and gd_ok:
                break

    print("Newton Result: x={}\nIter:{}\nabs:{}\n".format(
        output_nt, itr_n+1, abs_n))
    print("Gradient_Descent Result: x={}\nIter:{}\nabs:{}\n".format(
        output_gd, itr_g+1, abs_g))
```





#### (2.1). 完整代码

```python
import numpy as np
import matplotlib.pyplot as plt
import mpl_toolkits.axisartist as axisartist
import time
import math

#   User Parameters
initial = 0.999
delta_initial = 0.1
epsilon = 1e-5
max_itr = 50

def func(x): 
    return  x**3/3. - x

def derivative(x0,delta):
    return (func(x0+delta)-func(x0))/delta

for _ in range(999): 
    result = initial
    for itr in range(max_itr): 
        previous = initial
        value = func(initial)
        k = derivative(previous,1e-5)
        initial -= (value / k)
        itr+=1
        if (abs(func(initial) - 0) < epsilon) :
            break
    if (initial - 0 > epsilon):
        result -= delta_initial
    else:
        break

print("Result: x={}\n".format(result))
```



#### (2.2). 代码解读

主要思路为在第一问的代码基础上添加一层循环，如果给定的初始值不收敛于0，则改变初始值重新迭代。所以主要需要解决的问题有两点：

1. 初始值如何确定

   ![image-20210329224759287](C:\Users\nuaazs\AppData\Roaming\Typora\typora-user-images\image-20210329224759287.png)

   如上图可以看出，当上一时刻点处的值大于0时：

   - 方程切线斜率大于0，则点向左侧收敛。

   - 方程切线斜率小于0，则点向右侧收敛。

   值小于0情况相反。

   所以可以通过梯度下降法找寻极值来确定初始范围，最终为（-1,1)

   

2. 初始值更新的步长如何选择

   在上方代码中，我们选择的步长值为 0.1。但当问题更为复杂时，可以使用动态更新的步长来适当的减小迭代步数。
   
   对此题而言，可以用$$k_i$$来保存此时迭代结果，如果$$k_i$$变换一直大于容许值，则说明距离目标还较远，可以使用较大步长。如果$$k_i$$发生变换则减小$$step$$以获取更精确结果。
   
   ```python
   delta_initial = 0.1 # 初始选择较大步长
   ...
   new_ki = result
   ...
   if(abs(new_ki - ki) > epslion):
       delta_initial = delta_initial*0.1   # 将步长变为之前的十分之一
   ```
   





#### (3). 总结

1. 牛顿法相比于深度学习常用的梯度下降法，二者都是求解无约束最优化问题的常用方法，**牛顿法是二阶收敛，梯度下降法是一阶收敛，所以牛顿法更快。**

   牛顿法一般应用场景：

   求方程的根以及求解最优化方法；
   
   - 比如要求$$ f(x)=0 $$的根$$ x_0$$。计算相应的$$ f\left(x_{0}\right) $$零点的$$ f^{\prime}\left(x_{0}\right) $$，进行泰勒展开再取线性部分，也就是求方程：$$ f\left(x_{1}\right)-f\left(x_{0}\right)=f^{\prime}\left(x_{0}\right) \cdot\left(x_{1}-x_{0}\right) $$ 的解$$ x_{n+1}=x_{n}-\frac{f\left(x_{n}\right)}{f^{\prime}\left(x_{n}\right)} $$。 所以牛顿法求根是一阶算法。
   
   - 当我们将牛顿法用作优化算法的时候，它就是二阶的。假设我们有一个凸优化问题$$ \min _{x} f(x) $$：也就是说我们要找一个x来最小化$$f(x)$$。对于凸优化问题，$$f(x)$$的最小值点就是$$f(x)$$的极值点，也就是导数为0的点。那么我们上面的优化问题就转换为了如下的求根问题：$$ f^{\prime}(x)=0 $$。
   
     利用牛顿法求解上面的式子，我们先选取初始点x0，然后进行如下迭代：
     
     $$ x_{n+1}=x_{n}-\frac{f^{\prime}\left(x_{n}\right)}{f^{\prime \prime}\left(x_{n}\right)} $$ 直到 $$ x_{n+1}=x_{n}-\frac{f^{\prime}\left(x_{n}\right)}{f^{\prime \prime}\left(x_{n}\right)} $$。
     
     综上，牛顿法求根是一阶算法，我们将优化问题转为求根问题需要一阶导数，所以用牛顿法进行最优化是二阶算法。
   
   
   
2. 通过可以化对比也可发现牛顿法也有一些缺陷：
   1. 函数非凸时，这种情况下，牛顿法的**收敛性难以保证**；
   2. 即使是凸优化，**只有在迭代点离全局最优很近时**，牛顿法才会体现出收敛快的优势；
   3. 牛顿法**可能被鞍点吸引**

