## 1. 卷积与互相关

https://zhuanlan.zhihu.com/p/366472797

卷积是在信号处理、图像处理和其他工程/科学领域中广泛使用的技术。 在深度学习中，一种模型架构即卷积神经网络（CNN），以此技术命名。 然而，深度学习中的卷积本质上是信号/图像处理中的互相关，这两个操作之间存在细微差别。下面让我们来看一下两者之间的区别和联系。

在信号/图像处理中，卷积定义为：

![img](https://pic1.zhimg.com/80/v2-5951232006cf44d1b47659d61f16d529_1440w.jpg?source=1940ef5c)

它被定义为两个函数在反转和移位后的乘积的积分，以下可视化展示了这一过程：

![img](https://pic1.zhimg.com/80/v2-54e2424e6156ad8655416b42787beb8e_1440w.jpg?source=1940ef5c)

信号处理中的卷积：滤波器g反转，然后沿水平轴滑动。 对于每个位置，我们计算f和反向g之间的交叉面积，交叉区域是该特定位置的卷积值。

由以上可以看出，信号处理中的卷积是对滤波器进行反转位移后，再沿轴滑动，每一个滑动位置相交区域的面积，即为该位置的卷积值。

另一方面，互相关被称为**滑动点积**或两个函数的**滑动内积**。互相关的filters不需要反转，它直接在函数f中滑动。f和g之间的交叉区域是互相关，下图显示了相关性和互相关之间的差异：

![img](https://picx.zhimg.com/50/v2-8b5633fdcd11f76b21df95d9bd822282_720w.jpg?source=1940ef5c)

在深度学习中，卷积中的filters是不需要反转的。严格来说，它们是互相关的，**本质上是执行逐元素的乘法和加法**，在深度学习中我们称之为卷积。因为可以在训练过程中了解filters的权重。如果以上示例中的逆函数g是正确的函数，则在训练后，学习到的滤波器将看起来像逆函数g。因此，不需要像真正的卷积那样在训练之前先反转滤波器。



## 2. 深度学习中的卷积（单通道/多通道）

进行卷积的目的是从输入中提取有用的特征。在图像处理中，可以选择各种各样的filters。每种类型的filter都有助于从输入图像中提取不同的特征，例如水平/垂直/对角线边缘等特征。在卷积神经网络中，通过使用filters提取不同的特征，这些filters的权重是在训练期间自动学习的，然后将所有这些提取的特征“组合”以做出决策。

进行卷积有一些优势，例如**权重共享**和**平移不变性**。卷积还考虑了像素的空间关系，这些功能尤其有用，特别是在许多计算机视觉任务中，因为这些任务通常涉及识别某些组件与其他组件在空间上有一定的关系（例如，狗的身体通常连接到头部，四条腿和尾部）。

### 2.1 单通道卷积

在深度学习中，卷积本质上是对信号按元素相乘累加得到卷积值。对于具有1个通道的图像，下图演示了卷积的运算形式：

![动图封面](https://pic3.zhimg.com/50/v2-c21c79c6c0efcc38b9081ddbaffe24d8_720w.jpg?source=1940ef5c)

进行卷积的目的是从输入中提取有用的特征。在图像处理中，可以选择各种各样的filters。每种类型的filter都有助于从输入图像中提取不同的特征，例如水平/垂直/对角线边缘等特征。在卷积神经网络中，通过使用filters提取不同的特征，这些filters的权重是在训练期间自动学习的，然后将所有这些提取的特征进行组合，做出决策。

进行卷积有一些优势，例如权重共享和平移不变形。卷积还考虑了像素的空间关系，这些功能尤其有用，特别是在许多计算机视觉任务中因为这些任务通常设计识别某些组件与其他组件在空间上的一定关系（例如狗的身体连接到头部，四条腿和尾部）。



2.1 卷积：单通道形式

在深度学习中，卷积的本质是对信号按照元素相乘累加得到卷积值。对于具有1通道的图像。



2.2 卷积：多通道形式

在许多应用程序中，我们处理的是具有多个通道的图像，典型的例子是RGB图像。

多通道数据的另一个示例是卷积神经网络中的图层。卷积网络层通常包含多个通道（通常为数百个通道），每个通道描述了上一层中不同的特征。我们如何在不同深度的图层进行过度？我们如何将深度为n的图层转换为深度为m的图层？

在描述过程之前，我们想澄清一些术语：层，通道，特征图，filters和kernels。从层次结构的角度来看，层和filters的概念在同一级别，而通道和kernels在下面的一层。通道和特征图是一回事。图层可能具有多个通道（或特征图）：如果输入是RGB图像，则输入图层具有3个通道。**通常使用“通道”来描述“层”的结构。类似地，“kernels”用于描述“filters”的结构。**

filters和kernels之间的区别有点棘手，有时，它们可以互换使用，这可能会产生混淆。 从本质上讲，这两个术语有微妙的区别。“kernels”指的是2D-权重矩阵。 术语“filters”用于堆叠在一起的多个kernels的3D-结构。 对于2D-filters，filters与kernels相同。 但是对于3D-filters和深度学习中的大多数卷积而言，filters是kernels的集合。 每个kernels都是独一无二的，强调了输入通道的不同特征。



每个kernel应用到前一层的输入通道上，以生成一个输出通道。重复所有的kernels，生成多个通道。然后将这些通道中的每一个加在一起，形成耽搁输出通道。



可以将这个过程视为一个3D-filters矩阵滑动通过输入层。注意，输入层和filters深度都是相同的（即通道数=卷积核数）。这个3D-filters仅沿着2个方向（图像的高和宽）移动（这也是为什么3D-filters即使通常用于处理3D-体积数据，但这样的操作还是被称为2D-卷积）。

现在我们可以看到如何在不同深度的层之间实现过渡，假设输入层有Din个通道，而想让输出层的通道数量变为Dout，我们需要做的仅仅是将Dout个filter应用到输入层。每个filters都有Din个卷积核，都提供一个输出通道。在应用Dout个filters后，Dout个通道可以共同组成一个输出层。标准2D-卷积，通过使用Dout个filters，将深度为Din的层映射为另一个深度为Dout的层。

进一步的讲，对于卷积网络的设计，需要记住一下公式（2D卷积的计算）：

输入层：Win * Hin * Din

超参数：

filters个数：k

filters中卷积核维度：w*h

滑动步长（Stride）：s

填充值Padding：p

输出层: Wout * Hout* Dout

其中输出层和输入层的参数关系为：

Wout = (Win + 2p - w)/s + 1

Hout=(Hin+2p-h)/s+1

Dout =k

参数量为(w\*h\*Din+1)*k



3. 3D卷积

   在上一个插图中，可以看出，这实际上是在完成3D-卷积。但通常意义上，仍然称之为深度学习的2D-卷积。因为filters的深度和输入层的深度相同，3D-filters仅在2个维度上移动（图像的高度和宽度），得到的结果为单通道。

   通过将2D-卷积的推广，在3D-卷积定义为filters的深度小于输入层的深度（即卷积核的个数小于输入层的通道数），3D-filters需要在三个维度上滑动（输入层的长、宽、高）。在filters上滑动的每个位置执行一次卷积操作，得到一个数值。当filters滑过整个3D空间，输出的结构也是3D的。

2D-卷积和3D-卷积的主要区别为filters滑动的空间维度，3D-卷积的优势在于描述3D空间中的对象关系。3D关系在某一些应用中十分重要，如3D-对象的分割以及医学图像的重构等
