**千万不要小看PCA**， 很多人隐约知道求解最大特征值，其实并不理解PCA是对什么东西求解**特征值**和**特征向量**。  也不理解为什么是求解特征值和特征向量。 要理解到Hinton对PCA的认知，需要跨过4个境界。

**为什么要理解PCA？**

其实深度学习在成为深度学习以前，主要是**特征表达学习**， 而特征表达学习追溯到始祖象阶段，主要是无监督特征表达**PCA**和有监督特征表达LDA。  对了这里LDA不是主题模型的LDA，是统计鼻祖Fisher搞的linear discriminant analysis（参考“[Lasso简史](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247483709%26idx%3D1%26sn%3D127acc3a04d640dafaee7ae2b52db7ee%26chksm%3De8925536dfe5dc20ae7cde79bb6ac76e19730e7ca6fa8b8a9f75d3a6fcfffba58915a578786b%26scene%3D21%23wechat_redirect)”）。 而Hinton在这方面的造诣惊人， 这也是为什么他和学生一起能搞出牛牛的 t-Distributed Stochastic Neighbor Embedding (t-SNE) 。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/uxI82.jpg)

至于t-SNE为啥牛， 这里给两个对比图片， 然后我们再回到PCA，以后有机会再扩展！

t-SNE vs PCA： 可以看到线性特征表达的局限性

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/Lo9Os.jpg)

t-SNE 优于已有非线性特征表达 Isomap, LLE 和 Sammon mapping

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/SbsMI.jpg)



依然还记得2004年左右Isomap横空出世的惊奇， 再看t-SNE的诞生，真是膜拜！ 也正是Hinton对PCA能理解到他的境界， 他才能发明t-SNE。 



## PCA理解第一层境界：最大方差投影

正如PCA的名字一样， 你要找到主成分所在方向， 那么这个主成分所在方向是如何来的呢？

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/4EJKk.jpg)

其实是希望你找到一个垂直的新的坐标系， 然后投影过去， 这里有两个问题。 **第一问题**： 找这个坐标系的标准或者目标是什么？  **第二个问题**， 为什么要垂直的， 如果不是垂直的呢？   

如果你能理解第一个问题， 那么你就知道为什么PCA主成分是特征值和特征向量了。  如果你能理解第二个问题， 那么你就知道PCA和ICA到底有什么区别了。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/ALsbU.jpg)

对于第一个问题： 其实是要**求解方差最小或者最大**。 按照这个目标， 你代入拉格朗日求最值， 你可以解出来， 主成分方向，刚好是S的特征向量和特征值！ 是不是很神奇？  **伟大的拉格朗日**(参考 "[一步一步走向锥规划 - QP](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247484705%26idx%3D1%26sn%3Dc2805e6edbf45e0a72f6c597bd575b36%26chksm%3De892512adfe5d83c3599cc859a0c39c8b55eaa95dee312c386c00325e3ce3ec91c5ca17ffd82%26scene%3D21%23wechat_redirect)" "[一挑三 FJ vs KKT](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247484621%26idx%3D1%26sn%3Df0da49bf1923d44c6361a6cee6fe6f37%26chksm%3De89250c6dfe5d9d0dc8aa652c0cbc9d854e171018dc2f22d8eac597d0f0eeb780de90a6dc82f%26scene%3D21%23wechat_redirect) ")

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/ilRjy.jpg)

现在回答了，希望你理解了， PCA是对什么东西求解特征值和特征向量。 也理解为什么是求解的结果就是特征值和特征向量吧！

这仅仅是PCA的本意！ 我们也经常看到PCA用在图像处理里面， 希望用最早的主成分重建图像：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/ggFi6.jpg)

这是怎么做到的呢？

## PCA理解第二层境界：最小重建误差

什么是重建， 那么就是找个新的基坐标， 然后减少一维或者多维自由度。 然后重建整个数据。 好比你找到一个新的视角去看这个问题， 但是希望自由度小一维或者几维。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/jiwms.jpg)

那么目标就是要最小重建误差，同样我们可以根据最小重建误差推导出类似的目标形式。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/lnaV5.jpg)

虽然在第二层境界里面， 也可以直观的看成忽略了最小特征值对应的特征向量所在的维度。 但是你能体会到和第一层境界的差别么？ 一个是找主成分， 一个是维度缩减。 所以在这个层次上，才是把PCA看成降维工具的最佳视角。



## PCA理解第三层境界：高斯先验误差

在第二层的基础上， 如果**引入最小二乘法和带高斯先验的最大似然估计的等价性**。（参考"[一步一步走向锥规划 - LS](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247484399%26idx%3D1%26sn%3D1fa705b368c648c1f3a0cd5d9ef56eb8%26chksm%3De89257e4dfe5def243e9cd210883401f732662792d6bc64fc1c9811577552cc295d1ee69929d%26scene%3D21%23wechat_redirect)" “[最小二乘法的4种求解](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzMjU1NTg3Ng%3D%3D%26mid%3D2247485443%26idx%3D1%26sn%3D64733fcddcd02083095f72245bfbe059%26chksm%3De8925c08dfe5d51e413cd5199dfcf57b95b2425a3750b9885dfd78682447d83f2a2f148904b1%26scene%3D21%23wechat_redirect)” ） 那么就到了理解的第三层境界了。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/1mce1.jpg)

所以， 重最小重建误差， 我们知道求解最小二乘法， 从最小二乘法， 我们可以得到高斯先验误差。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/Ooe38.jpg)

有了高斯先验误差的认识，我们对PCA的理解， 进入了概率分布的层次了。 而正是基于这个概率分布层次的理解， 才能走到Hinton的理解境界。



## PCA理解第四层境界(Hinton境界)：线性流形对齐

如果我们把高斯先验的认识， 到到数据联合分布， 但是如果把数据概率值看成是空间。 那么我们可以直接到达一个新的空间认知。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/qRlPH.jpg)

这就是“Deep Learning”书里面写的， 烙饼空间（Pancake）， 而在烙饼空间里面找一个线性流行，就是PCA要干的事情。 我们看到目标函数形式和最小重建误差完全一致。 但是认知完全不在一个层次了。

