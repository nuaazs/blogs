## Bayes 贝叶斯分类法

优点：

1. 所需要估计的参数少，对缺失数据不敏感。
2. 有坚实的数学基础，以及稳定的分类效率。

缺点：

1. 需要假设属性之间相互独立，这往往并不成立。
2. 需要知道先验概率。
3. 分类决策存在错误率。



## Decision Tree 决策树

优点：

1. 不需要任何领域知识或参数假设。
2. 适合高维数据。
3. 简单、易于理解。
4. 能够在短时间内处理大量数据，得到可行且效果较好的结果。
5. 能够同时处理数据型和常规性属性。

缺点：

1. 对于各类别样本数量不一致的数据，信息增益偏向于那些具有更高多数量的样本。
2. 容易发生过拟合。
3. 忽略属性之间的相关性。
4. 不支持在线学习。



## SVM 支持向量机

优点：

1. 可以解决小样本下的机器学习的问题。
2. 提高泛化性能。
3. 可以决绝高纬、非线性问题。超高维文本分类仍受欢迎。
4. 避免神经网络结构选择和局部极小的问题。

缺点：

1. 对缺失数据敏感。
2. 内存消耗大。
3. 难以解释。
4. 运行速度慢，调参复杂。



## KNN（K近邻）

优点：

1. 核心思路简单，理论成熟，即可以用来做分类也可以用来做回归。
2. 可用于非线性分类。
3. 训练时间复杂度为O(n)。
4. 准确度高，对数据没有假设，对离群值不敏感。

缺点：

1. 计算量太大。
2. 对样本分类不均衡的问题，会产生误判。
3. 需要大量的内存。
4. 输出的可解释性不强。



## Logistic Regression 逻辑回归

优点：

1. 速度快。
2. 简单、易于理解，直接看到各个特征的权重。
3. 能容易地更新模型，吸收新的数据。
4. 能够动态调整分类阈值。

缺点：

1. 特征处理过程复杂，需要进行归一化等特征工程。



## Neural Network 神经网络

优点：

1. 分类准确率高。
2. 并行处理能力强。
3. 分布式存储和学习能力强。
4. 鲁棒性较强，不易受噪声影响。

缺点：

1. 需要大量参数（网络拓扑、阈值）
2. 难以解释结果。
3. 训练时间较长。



## Adaboosting 自适应增强算法

优点 ：

1. 有很高的精度。
2. 可以使用各种方法构建子分类器，Adaboosting提供的是框架。
3. 当使用简单分类器的时候，计算出的结果是可理解的，而且弱分类器构造及其简单。

4. 流程简单，不用做特征选择。
5. 不用担心过拟合。

缺点：

1. 对离群值比较敏感。