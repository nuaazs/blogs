# 从贝叶斯方法谈到贝叶斯网络

​																																																										**赵胜SX2006105**





## 1 贝叶斯方法



### 1.1 **频率派**与**贝叶斯**派各自不同的思考方式：

- 频率派把需要推断的参数$\theta$看做是固定的未知常数，即概率$\theta$虽然是未知的，但最起码是确定的一个值，同时，样本$X$是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本$X$ 的分布；
- 而贝叶斯派的观点则截然相反，他们认为参数$\theta$是随机变量，而样本$X$ 是固定的，由于样本是固定的，所以他们重点研究的是参数的$\theta$分布。

相对来说，频率派的观点容易理解，所以下文重点阐述贝叶斯派的观点。

贝叶斯派既然把$\theta$看做是一个随机变量，所以要计算$\theta$的分布，便得**事先知道$\theta$的无条件分布**，即在有样本之前（或观察到$X$之前），$\theta$有着怎样的分布呢？

比如往台球桌上扔一个球，这个球落会落在何处呢？如果是不偏不倚的把球抛出去，那么此球落在台球桌上的任一位置都有着相同的机会，即球落在台球桌上某一位置的概率$\theta$服从均匀分布。这种在实验之前定下的属于基本前提性质的分布称为**先验分布**，或$\theta$的无条件分布。

至此，贝叶斯及贝叶斯派提出了一个思考问题的固定模式：

​																					先验分布$$ \pi(\theta) $$ + 样本信息$$X $$ $$ \Rightarrow $$ 后验分布$$ \pi(\theta \mid x) $$


上述思考模式意味着，新观察到的样本信息将修正人们以前对事物的认知。换言之，在得到新的样本信息之前，人们对![img](https://img-blog.csdn.net/20141110210919359)的认知是先验分布![img](https://img-blog.csdn.net/20141110214925523)，在得到新的样本信息![img](https://img-blog.csdn.net/20141110211153578)后，人们对![img](https://img-blog.csdn.net/20141110210919359)的认知为![img](https://img-blog.csdn.net/20141110215058639)。**其中，先验信息一般来源于经验跟历史资料。**比如林丹跟某选手对决，解说一般会根据林丹历次比赛的成绩对此次比赛的胜负做个大致的判断。而**后验分布![img](https://img-blog.csdn.net/20141110215058639)一般也认为是在给定样本![img](https://img-blog.csdn.net/20141110211153578)的情况下![img](https://img-blog.csdn.net/20141110210919359)的条件分布，而使![img](https://img-blog.csdn.net/20141110215058639)达到最大的值![img](https://img-blog.csdn.net/20141110211547800)称为最大后验估计**，类似于经典统计学中的极大似然估计。

综合起来看，则好比是人类刚开始时对大自然只有少得可怜的先验知识，但随着不断观察、实验获得更多的样本、结果，使得人们对自然界的规律摸得越来越透彻。所以，贝叶斯方法既符合人们日常生活的思考方式，也符合人们认识自然的规律，经过不断的发展，最终占据统计学领域的半壁江山，与经典统计学分庭抗礼。此外，贝叶斯除了提出上述思考模式之外，还特别提出了举世闻名的贝叶斯定理。



### 1.2 贝叶斯定理

  在引出贝叶斯定理之前，先学习几个定义：

- **条件概率**（又称后验概率）就是事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为$P(A|B)$，读作“在B条件下A的概率”。

  比如，在同一个样本空间$\Omega$中的事件或者子集$A$与$B$，如果随机从$\Omega$中选出的一个元素属于$B$，那么这个随机选择的元素还属于$A$的概率就定义为在$B$的前提下$A$的条件概率，所以：$P(A|B) = |A∩B|/|B|$，接着分子、分母都除以$|\Omega|$得到
  
  ​																											$$ P(A \mid B)=\frac{P(A \cap B)}{P(B)} $$
  
- **联合概率**表示两个事件共同发生的概率。A与B的联合概率表示为$$ P(A \cap B) $$或者$$ P(A, B) $$
- **边缘概率**（又称先验概率）是某个事件发生的概率。边缘概率是这样得到的：在联合概率中，把最终结果中那些不需要的事件通过合并成它们的全概率，而消去它们（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率），这称为边缘化（marginalization），比如$A$的边缘概率表示为$P(A)$，$B$的边缘概率表示为$P(B)$。 

接着，考虑一个问题：$P(A|B)$是在$B$发生的情况下$A$发生的可能性。

1. 首先，事件$B$发生之前，我们对事件$A$的发生有一个基本的概率判断，称为$A$的先验概率，用$P(A)$表示；
2. 其次，事件$B$发生之后，我们对事件$A$的发生概率重新评估，称为$A$的后验概率，用$P(A|B)$表示；
3. 类似的，事件A发生之前，我们对事件$B$的发生有一个基本的概率判断，称为B的先验概率，用$P(B)$表示；
4. 同样，事件A发生之后，我们对事件$B$的发生概率重新评估，称为$B$的后验概率，用$P(B|A)$表示。

  贝叶斯定理便是基于下述贝叶斯公式：

​																											$$ P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)} $$

上述公式的推导其实非常简单，就是从条件概率推出。

根据条件概率的定义，在事件B发生的条件下事件A发生的概率是$$ P(A \mid B)=\frac{P(A \cap B)}{P(B)} $$,  同样地，在事件A发生的条件下事件B发生的概率 $$ P(B \mid A)=\frac{P(A \cap B)}{P(A)} $$.   整理与合并上述两个方程式，便可以得到：

​																						$$ P(A \mid B) P(B)=P(A \cap B)=P(B \mid A) P(A) $$

 接着，上式两边同除以P(B)，若P(B)是非零的，我们便可以得到**贝叶斯定理**的公式表达式：

​																									$$ P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)} $$

 所以，贝叶斯公式可以直接根据条件概率的定义直接推出。即因为$P(A,B) = P(A)P(B|A) = P(B)P(A|B)$，所以$P(A|B) = P(A)P(B|A)  / P(B)$。



### 1.3 应用：拼写检查

经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“[Julw](http://www.gu1234.com/search?hl=zh-CN&site=webhp&source=hp&q=Julw&btnK=Google+搜索&gws_rd=ssl)”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/32vkn.jpg)

 这叫做拼写检查。根据谷歌一员工写的[文章](http://norvig.com/spell-correct.html)显示，Google的拼写检查基于贝叶斯方法。下面我们就来看看，怎么利用贝叶斯方法，实现"拼写检查"的功能。

用户输入一个单词时，可能拼写正确，也可能拼写错误。如果把拼写正确的情况记做c（代表correct），拼写错误的情况记做w（代表wrong），那么"拼写检查"要做的事情就是：在发生w的情况下，试图推断出c。换言之：已知w，然后在若干个备选方案中，找出可能性最大的那个c，也就是求![img](https://img-blog.csdn.net/20141112230745208)的最大值。
而根据贝叶斯定理，有：$$ P(c \mid w)=P(w \mid c) * P(c) / P(w) $$

由于对于所有备选的c来说，对应的都是同一个w，所以它们的P(w)是相同的，因此我们只要最大化:$$ P(w \mid c) * P(c) $$  即可。其中：

- $P(c)$表示某个正确的词的出现"概率"，它可以用"频率"代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，P(c)就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。
- $P(w|c)$表示在试图拼写c的情况下，出现拼写错误w的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，P(w|c)就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。值得一提的是，一般把这种问题称为“编辑距离”，参见博客中的[这篇](http://blog.csdn.net/v_july_v/article/details/8701148#t4)文章。

所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。具体的计算过程及此方法的缺陷请参见[这里](http://norvig.com/spell-correct.html)。



## 2 贝叶斯网络



### 2.1 贝叶斯网络的定义

**贝叶斯网络**(Bayesian network)，又称信念网络(Belief Network)，或**有向无环图模型**(directed acyclic graphical model)，是一种概率图模型，于1985年由Judea Pearl首先提出。它是一种模拟人类推理过程中因果关系的不确定性处理模型，其网络拓朴结构是一个有向无环图(DAG)。 

贝叶斯网络的有向无环图中的**节点表示随机变量![img](https://img-blog.csdn.net/20141110221023930)**，它们可以是可观察到的变量，或隐变量、未知参数等。**认为有因果关系（或非条件独立）的变量或命题则用箭头来连接。**若两个节点间以一个单箭头连接在一起，表示其中一个节点是“因(parents)”，另一个是“果(children)”，两节点就会产生一个条件概率值。

总而言之，**连接两个节点的箭头代表此两个随机变量是具有因果关系，或非条件独立**。

例如，假设节点E直接影响到节点H，即E→H，则用从E指向H的箭头建立结点E到结点H的有向弧(E,H)，权值(即连接强度)用条件概率P(H|E)来表示，如下图所示：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/bCfrD.jpg)

简言之，把某个研究系统中涉及的随机变量，根据是否条件独立绘制在一个有向图中，就形成了**贝叶斯网络**。其主要用来描述随机变量之间的条件依赖，用**圈表示随机变量(random variables)**，用**箭头表示条件依赖(conditional dependencies)**。

令**G = (I,E)表示一个有向无环图(DAG)**，其中I代表图形中所有的节点的集合，而**E代表有向连接线段的集合**，且令**X = (Xi)i ∈ I为其有向无环图中的某一节点i所代表的随机变量**，若节点X的联合概率可以表示成：

​																											$$ p(x)=\prod_{i \in I} p\left(x_{i} \mid x_{\mathrm{pa}(i)}\right) $$

则称$X$为相对于一有向无环图$G$ 的贝叶斯网络，其中，$pa(i)$表示节点$i$之“因”，或称$pa(i)$是$i$的parents（父母）。

 此外，对于任意的随机变量，其联合概率可由各自的局部条件概率分布相乘而得出：

​																				$$ p\left(x_{1}, \ldots, x_{K}\right)=p\left(x_{K} \mid x_{1}, \ldots, x_{K-1}\right) \ldots p\left(x_{2} \mid x_{1}\right) p\left(x_{1}\right) $$

 如下图所示，便是一个简单的贝叶斯网络：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/TgtDE.jpg)

 因为a导致b，a和b导致c，所以有:
																									$$ p(a, b, c)=p(c \mid a, b) p(b \mid a) p(a) $$



### 2.2 贝叶斯网络的3种结构形式

给定如下图所示的一个贝叶斯网络：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/rrdU5.jpg)

 从图上可以比较直观的看出：

-  $x_1$,$x_2$,…$x_7$的联合分布为:

  $$ p\left(x_{1}\right) p\left(x_{2}\right) p\left(x_{3}\right) p\left(x_{4} \mid x_{1}, x_{2}, x_{3}\right) p\left(x_{5} \mid x_{1}, x_{3}\right) p\left(x_{6} \mid x_{4}\right) p\left(x_{7} \mid x_{4}, x_{5}\right) $$

- $x_1$和$x_2$独立（对应head-to-head）；
- $x_6$和$x_7$在$x_4$给定的条件下独立（对应tail-to-tail）。

根据上图，第1点可能很容易理解，但第2、3点中所述的条件独立是啥意思呢？其实第2、3点是贝叶斯网络中3种结构形式中的其中二种。为了说清楚这个问题，需要引入$D-Separation$（D-分离）这个概念。

$D-Separation$是一种用来判断变量是否条件独立的图形化方法。换言之，对于一个DAG(有向无环图)E，D-Separation方法可以快速的判断出两个节点之间是否是条件独立的。



#### 2.2.1 形式1：head-to-head

贝叶斯网络的第一种结构形式如下图所示：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/Qi5qj.jpg)

所以有：$P(a,b,c) = P(a)*P(b)*P(c|a,b)$成立，化简后可得：

$ \sum \mathrm{P}(\mathrm{a}, \mathrm{b}, \mathrm{c})=\sum_{\mathrm{c}} \mathrm{P}(\mathrm{a})^{*} \mathrm{P}(\mathrm{b}) * \mathrm{P}(\mathrm{c} \mid \mathrm{a}, \mathrm{b}) $
$ \Rightarrow P(a, b)=\mathrm{P}(\mathrm{a})^{*} \mathrm{P}(\mathrm{b}) $

 即在**c未知的条件下，a、b被阻断(blocked)，是独立的**，称之为$head-to-head$条件独立，对应本节中最开始那张图中的“$x_1$、$x_2$独立”。



#### 2.2.2 形式2：tail-to-tail

贝叶斯网络的第二种结构形式如下图所示:

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/n8LeX.jpg)



  考虑c未知，跟c已知这两种情况：

1. 在c未知的时候，有：$P(a,b,c)=P(c)*P(a|c)*P(b|c)$，此时，没法得出$P(a,b) = P(a)P(b)$，即c未知时，a、b不独立。
2. 在c已知的时候，有：$P(a,b|c)=P(a,b,c)/P(c)$，然后将$P(a,b,c)=P(c)*P(a|c)*P(b|c)$带入式子中，得到：$P(a,b|c)=P(a,b,c)/P(c) = P(c)*P(a|c)*P(b|c) / P(c) = P(a|c)*P(b|c)$，即c已知时，a、b独立。

所以，**在c给定的条件下，a，b被阻断(blocked)，是独立的**，称之为$tail-to-tail$条件独立，对应本节中最开始那张图中的“$x_6$和$x_7$在$x_4$给定的条件下独立”。



​																												 ![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/03Vlp.jpg)



#### 2.2.3 形式3：head-to-tail

 贝叶斯网络的第三种结构形式如下图所示：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/kYOPc.jpg)

 还是分c未知跟c已知这两种情况：

1. c未知时，有：$P(a,b,c)=P(a)*P(c|a)*P(b|c)$，但无法推出$P(a,b) = P(a)P(b)$，即c未知时，a、b不独立。

2. c已知时，有：$P(a,b|c)=P(a,b,c)/P(c)$，且根据$P(a,c) = P(a)*P(c|a) = P(c)*P(a|c)$，可化简得到：

   $$ \begin{aligned} & P(a, b \mid c) \\=& P(a, b, c) / P(c) \\=& P(a) * P(c \mid a) * P(b \mid c) / P(c) \\=& P(a, c) * P(b \mid c) / P(c) \\=& P(a \mid c) * P(b \mid c) \end{aligned} $$

    所以，**在c给定的条件下，a，b被阻断(blocked)，是独立的**，称之为$head-to-tail$条件独立。

  **插一句**：这个head-to-tail其实就是一个链式网络，如下图所示：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/qUJsL.jpg)

根据之前对$head-to-tail$的讲解，我们已经知道，在$x_i$给定的条件下，$x_{i+1}$的分布和$x_1$,$x_2$…$x_{i-1}$条件独立。意味着啥呢？意味着：$x_{i+1}$的分布状态只和$x_{i}$有关，和其他变量条件独立。通俗点说，当前状态只跟上一状态有关，跟上上或上上之前的状态无关。这种顺次演变的随机过程，就叫做马尔科夫链（Markov chain）。且有：

​																				$$ P\left(X_{n+1}=x \mid X_{0}, X_{1}, X_{2}, \ldots, X_{n}\right)=P\left(X_{n+1}=x \mid X_{n}\right) $$

接着，将上述结点推广到结 则是：对于任意的结点集$A$，$B$，$C$，考察所有通过$A$中任意结点到$B$中任意结点的路径，若要求$A$，$B$条件独立，则需要所有的路径都被阻断(blocked)，即满足下列两个前提之一：

1. $A$和$B$的“$head-to-tail$型”和“$tail-to-tail$型”路径都通过$C$；

2. $A$和$B$的“$head-to-head$型”路径不通过$C$以及$C$的子孙；

   ![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/gJpK8.jpg)

最后，举例说明上述$D-Separation$的3种情况（即贝叶斯网络的3种结构形式），则是如下图所示：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/h0OMb.jpg)

上图中左边部分是$head-to-tail$，给定$T$时，$A$ 和 $X$ 独立；右边部分的右上角是$tail-to-tail$，给定$S$时，$L$和$B$独立；右边部分的右下角是$head-to-head$，未给定$D$时，$L$和$B$独立。



### 2.3 贝叶斯网络的实例

给定如下图所示的贝叶斯网络：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/D9xkm.jpg)

  其中，各个单词、表达式表示的含义如下：

- **Smoking**表示吸烟，其概率用$P(S)$表示，**lung Cancer**表示的肺癌，一个人在吸烟的情况下得肺癌的概率用$P(C|S)$表示，**X-ray**表示需要照医学上的X光，肺癌可能会导致需要照X光，吸烟也有可能会导致需要照X光（所以Smoking也是X-ray的一个因），所以，因吸烟且得肺癌而需要照X光的概率用$P(X|C,S)$表示。
- **Bronchitis**表示支气管炎，一个人在吸烟的情况下得支气管炎的概率用$P(B|S)$，**Dyspnoea**表示呼吸困难，支气管炎可能会导致呼吸困难，肺癌也有可能会导致呼吸困难（所以lung Cancer也是Dyspnoea的一个因），因吸烟且得了支气管炎导致呼吸困难的概率用$P(D|C,B)$表示。

lung Cancer简记为C，Bronchitis简记为B，Dyspnoea简记为D，且C = 0表示lung Cancer不发生的概率，C = 1表示lung Cancer发生的概率，B等于0（B不发生）或1（B发生）也类似于C，同样的，D=1表示D发生的概率，D=0表示D不发生的概率，便可得到Dyspnoea的一张概率表，如上图的最右下角所示。

> 对于上图，在一个人已经呼吸困难（dyspnoea）的情况下，其抽烟（smoking）的概率是多少呢?

$ P( $ smoking $ \mid $ dyspnoea $ = $ yes $ )=? $



咱们来一步步计算推导下：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/IiwzG.jpg)



解释下上述式子推导过程：

1. 第二行：对联合概率关于$b$,$x$,$c$求和（在$d=1$的条件下），从而消去$b,x,c$，得到$s$和$d=1$的联合概率。
2. 第三行：最开始，所有变量都在$$ \sum $$的后面（$$ \sum $$表示对“求和”的称谓），但由于$P(s)$和“$d=1,b,x,c$”都没关系，所以，可以提到式子的最前面。而且$P(b|s)$和$x,c$没关系，所以，也可以把它提出来，放到$$ \sum_{b} $$的后面，从而式子的右边剩下$$ \sum_x $$和$$ \sum_c $$。

此外，图中$Variable elimination$表示的是变量消除的意思。为了更好的解决此类问题，咱们得引入因子图的概念。



#### 2.4.1 因子图的定义

 Wikipedia上是这样定义因子图的：**将一个具有多变量的全局函数因子分解，得到几个局部函数的乘积，以此为基础得到的一个双向图叫做因子图（Factor Graph）。**

比如，假定对于函数$$ g\left(X_{1}, X_{2}, \ldots, X_{n}\right) $$，有下述式子成立：

$$ g\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\prod_{j=1}^{m} f_{j}\left(S_{j}\right) $$

  其中$$ S_{j} \subseteq\left\{X_{1}, X_{2}, \ldots, X_{n}\right\} $$，其对应的因子图包$$ G=(X, F, E) $$括：

1. 变量节点$$ X=\left\{X_{1}, X_{2}, \ldots, X_{n}\right\} $$
2.  因子（函数）节点$$ F=\left\{f_{1}, f_{2}, \ldots, f_{m}\right\} $$
3. 边$E$，边通过下列因式分解结果得到：在因子（函数）节点$f_j$和变量节点$X_k$之间存在边的充要条件是存$$ X_{k} \in S_{j} $$存在。

正式的定义果然晦涩！我相信你没看懂。通俗来讲，所谓**因子图就是对函数进行因子分解得到的一种概率图**。一般内含两种节点：**变量节点**和**函数节点**。我们知道，**一个全局函数通过因式分解能够分解为多个局部函数的乘积，这些局部函数和对应的变量关系就体现在因子图上。**

举个例子，现在有一个全局函数，其因式分解方程为：

$$ g\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\right)=f_{A}\left(x_{1}\right) f_{B}\left(x_{2}\right) f_{C}\left(x_{1}, x_{2}, x_{3}\right) f_{D}\left(x_{3}, x_{4}\right) f_{E}\left(x_{3}, x_{5}\right) $$

 其中$f_A$,$f_B$,$f_C$,f$f_D$,$f_E$为各函数，表示变量之间的关系，可以是条件概率也可以是其他关系（如马尔可夫随机场Markov Random Fields中的势函数）。为了方便表示，可以写成：

$$ g(X)=\prod_{E \subseteq Q} f_{\varepsilon}\left(X_{\varepsilon}\right) $$

其对应的因子图为：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/5Z4fz.jpg)

且上述因子图等价于：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/yotLA.jpg)

所以，在因子图中，所有的**顶点不是变量节点就是函数节点**，**边线表示它们之间的函数关系**。

但搞了半天，虽然知道了什么是因子图，但因子图到底是干嘛的呢？为何要引入因子图，其用途和意义何在？事实上，因子图跟贝叶斯网络和马尔科夫随机场（Markov Random Fields）一样，也是**概率图**的一种。

既然提到了马尔科夫随机场，那顺便说下**有向图**、**无向图**，以及**条件随机场**等相关概念。

- 我们已经知道，有向图模型，又称作贝叶斯网络（Directed Graphical Models, DGM, Bayesian Network）。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/ZQllQ.jpg)

- 但在有些情况下，强制对某些结点之间的边增加方向是不合适的。使用没有方向的无向边，形成了无向图模型（Undirected Graphical Model,UGM）, 又被称为马尔科夫随机场或者马尔科夫网络（Markov Random Field,  MRF or Markov network）。

  ![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/OGM8G.jpg)

  

- 设$X=(X_1,X_2…X_n)$和$Y=(Y_1,Y_2…Y_m)$都是联合随机变量，若随机变量$Y$构成一个无向图$ G=(V,E)$表示的马尔科夫随机场（MRF），则条件概率分布$P(Y|X)$称为条件随机场（Conditional Random Field, 简称CRF。如下图所示，便是一个线性链条件随机场的无向图模型：

  ![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/p6cfG.jpg)

  

回到本文的主旨上来。在概率图中，**求某个变量的边缘分布**是常见的问题。这问题有很多求解方法，其中之一就是**把贝叶斯网络或马尔科夫随机场转换成因子图**，然后用$sum-product$算法求解。换言之，基于因子图可以用$sum-product $算法高效的求各个变量的边缘分布。

先通过一些例子分别说明如何把贝叶斯网络（和马尔科夫随机场），以及把马尔科夫链、隐马尔科夫模型转换成因子图后的情形，然后在2.4.2节，咱们再来看如何利用因子图的$sum-product$算法求边缘概率分布。

给定下图所示的贝叶斯网络或马尔科夫随机场：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/Om6wv.jpg)

根据各个变量对应的关系，可得：

$$ p(u, w, x, y, z)=p(u) p(w) p(x \mid u, w) p(y \mid x) p(z \mid x) $$

其对应的因子图为（以下两种因子图的表示方式皆可）：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/6tyhA.jpg)

由上述例子总结出由贝叶斯网络构造因子图的方法：

- 贝叶斯网络中的一个因子对应因子图中的一个结点
- 贝叶斯网络中的每一个变量在因子图上对应边或者半边
- 结点$g$和边$x$相连当且仅当变量$x$出现在因子$g$中。

  再比如，对于下图所示的由马尔科夫链转换而成的因子图：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/XnveJ.jpg)

$$ p_{X Y Z}(x, y, z)=p_{X}(x) p_{Y \mid X}(y \mid x) p_{Z \mid Y}(z \mid y) $$

 而对于如下图所示的由隐马尔科夫模型转换而成的因子图：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/4jmmW.jpg)

$$ p\left(x_{0}, x_{1}, x_{2}, \ldots, x_{n}, y_{1}, y_{2}, \ldots, y_{n}\right)=p\left(x_{0}\right) \prod_{k=1}^{n} p\left(x_{k} \mid x_{k-1}\right) p\left(y_{k} \mid x_{k-1}\right) $$



#### 2.4.2 Sum-product算法

我们已经知道，对于下图所示的因子图：

![image-20210112095839278](C:\Users\nuaazs\AppData\Roaming\Typora\typora-user-images\image-20210112095839278.png)

有$$ f\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\right)=f_{\mathrm{A}}\left(x_{1}, x_{2}, x_{3}\right) \cdot f_{\mathrm{B}}\left(x_{3}, x_{4}, x_{5}\right) \cdot f_{\mathrm{C}}\left(x_{4}\right) $$

下面，咱们来考虑一个问题：即如何由联合概率分布求边缘概率分布。

首先回顾下联合概率和边缘概率的定义，如下：

- **联合概率**表示两个事件共同发生的概率。A与B的联合概率表示为$$ P(A \cap B) $$或者$$ P(A, B) $$
- **边缘概率**（又称先验概率）是某个事件发生的概率。边缘概率是这样得到的：在联合概率中，把最终结果中那些不需要的事件通过合并成它们的全概率，而消去它们（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率），这称为边缘化（marginalization），比如A的边缘概率表示为P(A)，B的边缘概率表示为P(B)。 

  事实上，某个随机变量$f_k$的边缘概率可由$x_1$,$x_2$,$x_3$, ..., $x_n$的联合概率求到，具体公式为：$ \bar{f}_{k}\left(x_{k} \right) \triangleq \sum_{x_{1}, \ldots, x_{n},except\ x_{k}} f\left(x_{1}, \ldots, x_{n} \right) $

啊哈，啥原理呢？原理很简单，还是它：对$x_k$外的其它变量的概率求和，最终剩下$x_k$的概率！  此外，换言之，如果有$$ f\left(x_{1}, \ldots, f_{n}\right)=f_{1}\left(x_{1}\right) f_{2}\left(x_{2}\right) \cdots f_{n}\left(x_{n}\right) $$

那么:$$ \bar{f}_{k}\left(x_{k}\right)=\left(\sum_{x_{1}} f_{1}\left(x_{1}\right)\right) \cdots\left(\sum_{x_{k-1}} f_{k-1}\left(x_{k-1}\right)\right) f_{k}\left(x_{k}\right) \cdots\left(\sum_{x_{n}} f_{n}\left(x_{n}\right)\right) $$

上述式子如何进一步化简计算呢？考虑到我们小学所学到的乘法分配率，可知a*b + a*c = a*(b + c)，前者2次乘法1次加法，后者1次乘法，1次加法。我们这里的计算是否能借鉴到分配率呢？别急，且听下文慢慢道来。

假定现在我们需要计算如下式子的结果：

$ \bar{f}_{3}\left(x_{3}\right)=\sum_{x_{1}, \ldots, x_{7}\ except\ x_{3}} f\left(x_{1}, \ldots, x_{7}\right) $

同时，$f$ 能被分解如下：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/IhZhl.jpg)

借鉴分配率，我们可以提取公因子：

![image-20210112100433962](C:\Users\nuaazs\AppData\Roaming\Typora\typora-user-images\image-20210112100433962.png)

因为变量的边缘概率等于所有与他相连的函数传递过来的消息的积，所以计算得到：

$$ \begin{aligned} \bar{f}_{3}\left(x_{3}\right)=&\left(\sum_{x_{1}, x_{2}} f_{1}\left(x_{1}\right) f_{2}\left(x_{2}\right) f_{3}\left(x_{1}, x_{2}, x_{3}\right)\right) \\ & \cdot\left(\sum_{x_{4}, x_{5}} f_{4}\left(x_{4}\right) f_{5}\left(x_{3}, x_{4}, x_{5}\right)\left(\sum_{x_{6}, x_{7}} f_{6}\left(x_{5}, x_{6}, x_{7}\right) f_{7}\left(x_{7}\right)\right)\right) \end{aligned} $$

 仔细观察上述计算过程，可以发现，其中用到了类似“消息传递”的观点，且总共两个步骤。

- 第一步:对于$f$ 的分解图，根据蓝色虚线框、红色虚线框围住的两个box外面的消息传递：

  ![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/Mx5kg.jpg)

  

  

  计算可得：$$ \begin{aligned} \bar{f}_{3}\left(x_{3}\right)=&(\underbrace{\sum_{x_{1}, x_{2}} f_{1}\left(x_{1}\right) f_{2}\left(x_{2}\right) f_{3}\left(x_{1}, x_{2}, x_{3}\right)}_{\vec{\mu}_{X_{3}}\left(x_{3}\right)}) \\ & \cdot\left(\sum_{x_{4}, x_{5}} f_{4}\left(x_{4}\right) f_{5}\left(x_{3}, x_{4}, x_{5}\right)\left(\sum_{x_{6} x_{7}} f_{6}\left(x_{5}, x_{6}, x_{7}\right) f_{7}\left(x_{7}\right)\right)\right) \end{aligned} $$

  

  ![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/xNVqN.jpg)

-  第二步:根据蓝色虚线框、红色虚线框围住的两个box内部的消息传递：

 根据$$ \vec{\mu}_{X_{1}}\left(x_{1}\right) \triangleq f_{1}\left(x_{1}\right), \vec{\mu}_{X_{2}}\left(x_{2}\right) \triangleq f_{2}\left(x_{2}\right) $$, 我们有：

$ \vec{\mu}_{X_{3}}\left(x_{3}\right)=\sum_{x_{1}, x_{2}} \vec{\mu}_{X_{1}}\left(x_{1}\right) \vec{\mu}_{X_{2}}\left(x_{2}\right) f_{3}\left(x_{1}, x_{2}, x_{3}\right) $
$ \overleftarrow{\mu}_{X_{5}}\left(x_{5}\right)=\sum_{x_{6}, x_{7}} \vec{\mu}_{X_{7}}\left(x_{7}\right) f_{6}\left(x_{5}, x_{6}, x_{7}\right) $
$ \overleftarrow{\mu}_{X_{3}}\left(x_{3}\right)=\sum_{x_{4}, x_{5}} \vec{\mu}_{X_{4}}\left(x_{4}\right) \overleftarrow{\mu}_{X_{5}}\left(x_{5}\right) f_{5}\left(x_{3}, x_{4}, x_{5}\right) $



就这样，上述计算过程将一个概率分布写成两个因子的乘积，而这两个因子可以继续分解或者通过已知得到。这种利用消息传递的观念计算概率的方法便是$sum-product$算法。前面说过，基于因子图可以用$sum-product$算法可以高效的求各个变量的边缘分布。

到底什么是$sum-product$算法呢？$sum-product$算法，也叫$belief propagation$，有两种消息：

- 一种是变量(Variable)到函数(Function)的消息：![img](https://img-blog.csdn.net/20141112163109597)，如下图所示

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/qZFFs.jpg)

​		此时，变量到函数的消息为![img](https://img-blog.csdn.net/20141112164737703)。

- 另外一种是函数(Function)到变量(Variable)的消息：![img](https://img-blog.csdn.net/20141112163122468)。如下图所示：

  ![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/87UHL.jpg)

此时，函数到变量的消息为：![img](https://img-blog.csdn.net/20141112164838031)。  以下是sum-product算法的总体框架：

- *1*、给定如下图所示的因子图：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/soqAi.jpg)

- *2*、Sum-product 算法的消息计算规则为：

$$ \vec{\mu}_{X}(x)=\sum_{y_{1}, \ldots, y_{n}} g\left(x, y_{1}, \ldots, y_{n}\right) \vec{\mu}_{Y_{1}}\left(y_{1}\right) \cdots \vec{\mu}_{Y_{n}}\left(y_{n}\right) $$

- *3*、根据sum-product定理，如果因子图中的函数$f$ 没有周期，则有：

$$ \bar{f}_{X}(x)=\vec{\mu}_{X}(x) \overleftarrow{\mu}_{X}(x) $$



值得一提的是：如果因子图是无环的，则一定可以准确的求出任意一个变量的边缘分布，如果是有环的，则无法用sum-product算法准确求出来边缘分布。

比如，下图所示的贝叶斯网络：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/xfrYR.jpg)

 其转换成因子图后，为：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/1SuDQ.jpg)

可以发现，若贝叶斯网络中存在“环”（无向），则因此构造的因子图会得到环。而使用消息传递的思想，这个消息将无限传输下去，不利于概率计算。
  解决方法有3个：

- 1、删除贝叶斯网络中的若干条边，使得它不含有无向环

  比如给定下图中左边部分所示的原贝叶斯网络，可以通过去掉C和E之间的边，使得它重新变成有向无环图，从而成为图中右边部分的近似树结构

  ![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/knCDG.jpg)

  

  具体变换的过程为最大权生成树算法MSWT（详细建立过程请参阅此[PPT](http://pan.baidu.com/s/1o69Lp1K) 第60页），通过此算法，这课树的近似联合概率P'(x)和原贝叶斯网络的联合概率P(x)的相对熵（如果忘了什么叫相对熵，请参阅：[最大熵模型中的数学推导](http://blog.csdn.net/v_july_v/article/details/40508465)）最小。

- 2、重新构造没有环的贝叶斯网络

- 3、选择loopy belief propagation算法（你可以简单理解为sum-product 算法的递归版本），此算法一般选择环中的某个消息，随机赋个初值，然后用sum-product算法，迭代下去，因为有环，一定会到达刚才赋初值的那个消息，然后更新那个消息，继续迭代，直到没有消息再改变为止。唯一的缺点是不确保收敛，当然，此算法在绝大多数情况下是收敛的。

此外，除了这个sum-product算法，还有一个max-product 算法。但只要弄懂了sum-product，也就弄懂了max-product 算法。因为max-product 算法就在上面sum-product 算法的基础上把求和符号换成求最大值max的符号即可！

最后，sum-product 和 max-product 算法也能应用到隐马尔科夫模型hidden Markov models上，后面有机会的话可以介绍。



本文完。

