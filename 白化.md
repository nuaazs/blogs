白化是一种重要的预处理过程，其目的就是**降低输入数据的冗余性**，使得经过白化处理的输入数据具有如下性质：

1. 特征之间相关性较低
2. 所有特征具有相同的方差

 

白化处理分**PCA白化**和**ZCA白化**.

**<u>PCA白化保证数据各维度的方差为1</u>**，**<u>而ZCA白化保证数据各维度的方差相同</u>**。PCA白化可以用于降维也可以去相关性，而ZCA白化主要用于去相关性，且尽量使白化后的数据接近原始输入数据。





## **1. PCA白化**



 根据白化的**两个要求**，我们首先是降低特征之间的相关性。在PCA中，我们选取**前K大的特征值的特征向量**作为投影方向，如果K的大小为数据的维度n，把这K个特征向量组成选择矩阵U（每一列为一个特征向量），![img](https://img-blog.csdn.net/20141225155006251)为旋转后的数据。**如果K<n，就是PCA降维**，**如果K=n，则降低特征间相关性降低**。

![img](https://img-blog.csdn.net/20141225155011165)

![img](https://img-blog.csdn.net/20141225155014909)

上图显示了原始数据和经过PCA旋转之后的数据，可以发现数据之间的相对位置都没有改变，仅仅改变了数据的基，但这种方法就降低了数据之后的相关性。（原始数据的相关性为正，因为x1增加时，x2也增加；而处理之后的数据的相关性明显降低）

 

第二个要求是每个输入特征具有单位方差，以直接使用![img](https://img-blog.csdn.net/20141225155024862)作为缩放因子来缩放每个特征![img](https://img-blog.csdn.net/20141225155018703)，计算公式![img](https://img-blog.csdn.net/20141225155036952)，经过PCA白化处理的数据分布如下图所示，此时的协方差矩阵为单位矩阵I。

 ![img](https://img-blog.csdn.net/20141225155042927)

![img](https://img-blog.csdn.net/20141225155045390)

​        代码实现：

```python
#-*- coding:utf-8-*-
from numpy import *
import numpy as np
import cv2
import matplotlib.pyplot as plt

#机器学习实战
def loadDataSet(filename,delim = "\t"):
    fr = open(filename)
    stringArr = [line.strip().split(delim) for line in fr.readlines()]
    datArr = [map(float, line) for line in stringArr]
    return np.mat(datArr)
def PCA_whitening(dataMat):
    newData, meanVals = zeroMean(dataMat)
    covMat = np.cov(newData, rowvar=0)
    U, S, V = np.linalg.svd(covMat)  
# U 是 covMat 的特征向量矩阵，S 是其特征值矩阵；因为 covMat 是对称方#  阵，所以 V=U'，covMat=USV
    rotateMat=newData*U
    rotateMat/=sqrt(S+1e-5)
    return rotateMat
 
data=loadDataSet("F:/DEEP LEARNING PAPERS/machinelearninginaction/Ch13/testSet.txt")
	lowdataMat=PCA_whitening(data)
fig1 = plt.figure()
fig2=plt.figure()
ax = fig1.add_subplot(111)
ay=fig2.add_subplot(111)
ax.scatter(data[:,0].flatten().A[0], data[:,1].flatten().A[0], marker='^',  s = 10 )
ay.scatter(lowdataMat[:,0].flatten().A[0], lowdataMat[:,1].flatten().A[0],marker='o', s = 20 , c ='red' )
```





## **2. ZCA白化**



ZCA白化的定义为：

​     ![img](https://img-blog.csdn.net/20141225155452917)

**ZCA白化只是在PCA白化的基础上做了一个旋转操作，使得白化之后的数据更加的接近原始数据。**

ZCA白化首先通过PCA去除了各个特征之间的相关性，然后是输入特征具有单位方差，此时得到PCA白化后的处理结果，然后再把数据旋转回去，得到ZCA白化的处理结果，感觉这个过程让数据的特征之间有具有的一定的相关性，下面实验进行验证。

 

​        代码实现：

```python
def ZCA_whitening(data):
    newData, meanVals = zeroMean(data)
    covMat = np.cov(newData, rowvar=0)
    U, S, V = np.linalg.svd(covMat)  # U 是 covMat 的特征向量矩阵，S 是其特征值矩阵；因为 covMat 是对称方阵，所以 V=U'，covMat=USV
    rotateMat = newData * U
    rotateMat /= sqrt(S + 1e-5)
 
    return (U*rotateMat.T).T
       
data=loadDataSet("F:/DEEP LEARNING PAPERS/machinelearninginaction/Ch13/testSet.txt")
lowdataMat=ZCA_whitening(data)
fig1 = plt.figure()
fig2=plt.figure()
ax = fig1.add_subplot(111)
ay=fig2.add_subplot(111)
ax.scatter(data[:,0].flatten().A[0], data[:,1].flatten().A[0], marker='^',  s = 10 )
ay.scatter(lowdataMat[:,0].flatten().A[0], lowdataMat[:,1].flatten().A[0],marker='o', s = 20 , c ='red' )
plt.show()
 
```





## **3. PCA白化和ZCA白化的区别**



PCA白化ZCA白化都降低了特征之间相关性较低，同时使得所有特征具有相同的方差。

1. PCA白化需要保证数据各维度的方差为1，ZCA白化只需保证方差相等。

2. PCA白化可进行降维也可以去相关性，而ZCA白化主要用于去相关性。

3. ZCA白化相比于PCA白化使得处理后的数据更加的接近原始数据。





## **4. 正则化**



实践中需要实现PCA白化或ZCA白化时，有时一些特征值![img](https://img-blog.csdn.net/20141225155716515)在数值上接近于0，这样在缩放步骤时我们除以![img](https://img-blog.csdn.net/20141225155718875)将导致除以一个接近0的值，这可能使数据上溢 (赋为大数值)或造成数值不稳定。因而在实践中，我们使用少量的正则化实现这个缩放过程，即在取平方根和倒数之前给特征值加上一个很小的常数![img](https://img-blog.csdn.net/20141225155723333) ：



当x在区间 [-1,1] 上时, 一般取值为![img](https://img-blog.csdn.net/20141225155726609)。