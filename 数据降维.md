## 1. 缺失值比率（Missing Value Ratio）

## 2. 低方差滤波（Low Variance Filter）

## 3. 高相关滤波（High Correlation filter）

### SelectKBest -- 过滤式

```python
#coding:utf-8
from sklearn import feature_selection
import numpy as np
import matplotlib.pyplot as plt
import sys


from scipy.sparse import issparse
from sklearn.preprocessing import LabelBinarizer
from sklearn.utils.extmath import safe_sparse_dot, row_norms
from scipy import special, stats
import scipy as scipy

def chisquare(f_obs, f_exp):
    """Fast replacement for scipy.stats.chisquare.
    """
    f_obs = np.asarray(f_obs, dtype=np.float64)

    k = len(f_obs)#3
    # Reuse f_obs for chi-squared statistics
    chisq = f_obs#观测值，实际值

    chisq -= f_exp#实际值-期望值

    chisq **= 2

    with np.errstate(invalid="ignore"):
        chisq /= f_exp

    chisq = chisq.sum(axis=0)

    #得到卡方值和p值，这里的卡方计算和一般不太一样，p值作用是什么？
    return chisq, special.chdtrc(k - 1, chisq)

def chi2(X, y):
    """Compute chi-squared stats between each non-negative feature and class.
    计算每个非负特征和类之间的 卡方 统计量
    """
    Y = LabelBinarizer().fit_transform(y)
    #print(Y)#[[1 0 0], [0 1 0],[0 0 1]]为什么要变成这种形式？

    observed = safe_sparse_dot(Y.T, X)# n_classes * n_features
    #print(np.sum(observed,axis=0))#和为总体样本的和（各个维度）

    feature_count = X.sum(axis=0).reshape(1, -1)#计算每个属性上样本总和
    class_prob = Y.mean(axis=0).reshape(1, -1)#根据每个类别样本数量--计算每个类别比重(50/150,50/150)
    #print(feature_count)
    #print(class_prob)#用概率和样本总和相乘

    expected = np.dot(class_prob.T, feature_count)

    #observed, expected的--列向--总和都是和所有样本的--列向--总和相等
    #observed--实际值，也就是每个类别样本值真的相加得到
    #expected--理论值，用每个类别数在总样本数中比例--乘以--样本总和
    #相等的情况：这种情况确实存在--但是很难......(一个属性时后得到！！！！！)
    #没有考虑属性值的卡方计算
    return chisquare(observed, expected)#观测，期望

def mySelectKBest(data,label,score_func,n):
    '''
    包裹式--选择一种打分函数，然后分别计算每个属性对应得分
    :param n:
    :return:
    '''
    score_func_ret = score_func(data, label)
    if isinstance(score_func_ret, (list, tuple)):
        scores, pvalues = score_func_ret
        pvalues = np.asarray(pvalues)
    else:
        scores = score_func_ret
        pvalues = None

    scores = np.asarray(scores)
    #主体是得分计算函数--chi2(卡方)--然后根据得分选择前k个特征即可
    dataNew = data[:,np.argsort(scores)[::-1][0:n]]

    print(dataNew)
    return dataNew

if __name__ == '__main__':
    data = load_iris().data  # shape=(150,4)
    label = load_iris().target
    mySelectKBest(data,label,chi2,2)
```





### RFE（RFECV）--伪包裹式

```python
#coding:utf-8
import numpy as np
from sklearn import tree
from sklearn.datasets import load_iris
import sys

from sklearn.utils import check_X_y, safe_sqr
from sklearn.base import clone

def fit(X, y, estimator, n_features_to_select=None,step=1,step_score=None,verbose=0):

    X, y = check_X_y(X, y, "csc")
    # Initialization
    n_features = X.shape[1]#4个特征(属性)

    #确定--保留特征--个数
    if n_features_to_select is None:
        n_features_to_select = n_features // 2#如果没有指定--则定为原特征数一半
    else:
        n_features_to_select = n_features_to_select
    if 0.0 < step < 1.0:
        step = int(max(1, step * n_features))#以比例形式，确定step
    else:
        step = int(step)
    support = np.ones(n_features, dtype=np.bool)#每个特征的最终选择情况
    ranking = np.ones(n_features, dtype=np.int)#每个特征的最终优先级
    
    if step_score:
        scores_ = []
        
    # Elimination
    while np.sum(support) > n_features_to_select:#当前特征个数--大于--最终要保留的特征个数
        # Remaining features
        features = np.arange(n_features)[support]

        # Rank the remaining features
        estimator = clone(estimator)

        estimator.fit(X[:, features], y)#学习器喂入数据
        #学习器必须是可以产生每个维度重要程度的学习器
        # Get coefs
        if hasattr(estimator, 'coef_'):
            coefs = estimator.coef_
        else:
            coefs = getattr(estimator, 'feature_importances_', None)
        #print(coefs)#--压根就没有用到“包裹式”的核心思想--本质仍然是--过滤式

        # Get ranks
        if coefs.ndim > 1:
            ranks = np.argsort(safe_sqr(coefs).sum(axis=0))
        else:
            ranks = np.argsort(safe_sqr(coefs))
        #safe_sqr(coefs)#对元素做平方操作--为什么平方之后再排序？属性“重要程度”真的会有负值？

        # for sparse case ranks is matrix
        ranks = np.ravel(ranks)

        # Eliminate the worse features
        threshold = min(step, np.sum(support) - n_features_to_select)

        print('特征重要性排序后的下标:')
        print(ranks)#特征重要性排序后的下标

        support[features[ranks][:threshold]] = False
        print('每个特征的最终选择情况:')
        print(support)
        ranking[np.logical_not(support)] += 1
        print(ranking)

    # Set final attributes
    features = np.arange(n_features)[support]
    estimator_ = clone(estimator)
    estimator_.fit(X[:, features], y)

    # Compute step score when only n_features_to_select features left
    if step_score:
        scores_.append(step_score(estimator_, features))
    n_features_ = support.sum()
    support = support
    ranking = ranking

if __name__ == '__main__':
    # 数据集还是用鸢尾花数据集
    data = load_iris().data  # shape=(150,4)
    label = load_iris().target
    estimator = tree.DecisionTreeClassifier()#学习器--选择的是--分类树
    fit(data,label,estimator,n_features_to_select=2,step=1)
```



### SelectFromModel--半嵌入式

这个嵌入式怎么说呢，也是利用**学习器的coef属性（或者feature_importances）**，从这点来看似乎和上面的伪包裹式相同。但是，如果**使用线性模型，尤其是带L1的线性模型**，它的思想就契合与嵌入式特征选择的理论了。因为在**使用鸢尾花数据集测试时，确实得到了权重为0的特征**。这很让人开心。

```python
#coding:utf-8
import numpy as np
import sys
from sklearn.datasets import load_iris
from sklearn.svm import LinearSVC
import matplotlib.pyplot as plt

from sklearn.base import clone
from sklearn.externals import six
from sklearn.exceptions import NotFittedError
from sklearn.utils.metaestimators import if_delegate_has_method
from sklearn.utils import check_array, safe_mask

def get_feature_importances(estimator, norm_order=1):
    """Retrieve or aggregate feature importances from estimator"""

    #尝试--获取estimator对象的--feature_importances_属性值--如果没获取到--返回None
    importances = getattr(estimator, "feature_importances_", None)

    if importances is None and hasattr(estimator, "coef_"):
        #"feature_importances_"--和--"coef_"，都是学习器自带的呀--这里用来表征--特征重要程度！！！
        if estimator.coef_.ndim == 1:
            importances = np.abs(estimator.coef_)

        else:
            #每个类别--每个属性--都有一个权重，将不同类别同一属性权重相加--即为该维度的--重要程度得分
            #线性模型--属性权重绝对值--为什么对应该属性重要程度？？？
            importances = np.linalg.norm(estimator.coef_, axis=0,ord=norm_order)

    return importances

def _calculate_threshold(estimator, importances, threshold):
    """Interpret the threshold value
    计算特征选择时--得分阈值
    """

    if threshold is None:
        # determine default from estimator
        est_name = estimator.__class__.__name__

        #不同的--学习器--对应的阈值计算不同
        if ((hasattr(estimator, "penalty") and estimator.penalty == "l1") or
                "Lasso" in est_name):

            threshold = 1e-5#默认值
        else:
            threshold = "mean"#采用均值计算方式

    if isinstance(threshold, six.string_types):
        print(threshold)
        if "*" in threshold:
            print(threshold)
            scale, reference = threshold.split("*")
            scale = float(scale.strip())
            reference = reference.strip()

            if reference == "median":
                reference = np.median(importances)
            elif reference == "mean":
                reference = np.mean(importances)
            else:
                raise ValueError("Unknown reference: " + reference)

            threshold = scale * reference

        elif threshold == "median":
            threshold = np.median(importances)

        elif threshold == "mean":
            threshold = np.mean(importances)

    else:#阈值可以人为设定（按照“嵌入式基本思想”--属性得分大于0--就应该保留）
        threshold = float(threshold)

    return threshold

class SelectFromModel():
    """Meta-transformer for selecting features based on importance weights.
    """
    def __init__(self, estimator, threshold=None, norm_order=1):
        self.estimator = estimator
        self.threshold = threshold
        self.norm_order = norm_order

    def get_support_mask(self):
        # SelectFromModel can directly call on transform.
        estimator = self.estimator#学习器

        #调用第一个函数
        #得到每个--维度--对应重要程度得分
        scores = get_feature_importances(estimator, self.norm_order)
        #print(scores)#这里得到score，其实就已经决定了最终结果

        #调用第二个函数
        #得到--重要程度得分选取--阈值--大于该阈值的特征被保留
        threshold = _calculate_threshold(estimator, scores, self.threshold)

        # 这里就已经确定了--后面要选择特征的个数和对应维度
        return scores >= threshold

    def transform(self, X):
        X = check_array(X, accept_sparse='csr')
        mask = self.get_support_mask()#通过一系列函数已经得到了--最终要选择的维度
        #print(mask)#[ True  True  True False]

        return X[:, safe_mask(X, mask)]

if __name__ == '__main__':
    iris = load_iris()
    X, y = iris.data, iris.target

    lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
    #coef_ :权重  intercept_:偏移量

    model = SelectFromModel(lsvc)
    X_new = model.transform(X)
    print(X_new)
```