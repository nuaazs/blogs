## **为什么你的模型效果这么差？**

为什么别人的模型都能快速达到较低的错误率，而你的模型错误率却居高不下。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/faUnQ.jpg)

造出这种现象的原因可以分为4大类：

1、模型实现中的bug：比如标签错误的问题。

2、超参数选择不合适：模型对超参数很敏感，学习率太高或太低都不行。

3、数据模型不适配：比如你要训练一个自动驾驶图像识别的模型，用ImageNet数据集来训练就不合适。

4、数据集的构造问题：没有足够数据、分类不均衡、有噪声的标签、训练集合测试集分布不同。



## **深度学习debug的流程策略**

针对上面的问题，小哥总结出调试深度学习模型的第一要义——**悲观主义**。

既然消除模型中的错误很难，我们不如先从简单模型入手，然后逐渐增加模型的复杂度。

他把这个过程分为5个步骤：

1. 从最简单模型入手；
2. 成功搭建模型，重现结果；
3. 分解偏差各项，逐步拟合数据；
4. 用由粗到细随机搜索优化超参数；
5. 如果欠拟合，就增大模型；如果过拟合，就添加数据或调整。



## **从简单模型开始**

在这一步之前，假定你已经有了初始的测试集、需要改进的单一指标、基于某种标准的模型目标性能。

首先，选择一个简单的架构。比如，你的输入是图片就选择类似LeNet的架构，输入是语言序列就选择有一个隐藏层的LSTM。

模型推荐的默认设置：

- Adam优化器学习速率3e-4
- 激活函数选用ReLU或tanh
- ReLU初始化推荐使用He normal，tanh初始化推荐使用Glorot normal

为了简化问题，我们从一个只有1万样本的数据集开始训练，数据的特点包括：固定数量的目标、分类、更小的图片尺寸。由此创建一个简单的合成训练集。



## **开始搭建深度学习模型**

5种最常见的bug：

错误的张量形状；预处理输入错误；损失函数错误输入；忘记设置正确的训练模型；错误的数据类型。

为了防止这些错误发生：尽可能减少代码的行数，使用现成的组件，然后再构建复杂的数据pipeline。

运行模型后，你可能会遇到形状不匹配、数据类型错误、内存不足等等问题。

对于第一个问题，可以在调试器中逐步完成模型创建和推理。数据类型错误是由于没有把其他类型数据转化成float32，内存不足是因为张量或者数据集太大。



## **评估**

下面我们开始用错误率评估模型的性能。

**测试集错误率 = 错误率下限 + 偏移 + 方差 + 分布偏差 + 验证集过拟合**

为了处理训练集和测试集分布的偏差，我们使用两个验证数据集，一个样本来自训练集，一个样本来自测试集。

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/9Pa9n.jpg)

## **改进模型和数据** 

上一步中粗略搭建的模型错误率仍然相当高，我们应该如何改进？

让我们先用以下方法解决欠拟合的问题：

**让模型更大**（比如加入更多的层，每层中使用更多的单元）；**减少正规化**；**错误分析**；**选择另一种性能更好的模型架构**；**调节超参数**；**加入更多特征**。

首先，我们给模型加入更多的层，转换到ResNet-101，调节学习率，使训练集错误率降低到0.8%。

在出现过拟合后，我们可以增加训练集的样本量解决这个问题，把图片数量扩大到25万张。

经历过优化参数、权重衰减、数据增强等一系列操作后，我们终于把测试错误率降低到目标值。

接下来我们着手解决训练集和测试集的分布偏差问题。分析测试验证集错误率，收集或者合成更多训练数据弥补二者的偏差。比如下面的自动驾驶目标识别模型，训练完成后，让它判断图片里有没有人，常常发生错误。



经过分析得出，训练集缺乏夜晚场景、反光等情况。后续将在训练集中加入此类数据纠正偏差。

另一种修正错误率的方法称为领域适配，这是一种使用未标记或有限标记数据进行训练的技术。它能在源分布上进行训练，并将其推广到另一个“目标”。



## **超参数优化**

这是调试的最后一步，我们需要选取那些更敏感的超参数，下图是模型对不同超参数的敏感性：

![](https://shengbucket.oss-cn-hangzhou.aliyuncs.com/pics/3EZWT.jpg)

常用的超参数优化方法有：手动优化、网格搜索、随机搜索、由粗到细、贝叶斯优化。



你可以手动优化超参数，但是耗时而且需要理解算法的细节。Josh推荐的方法是**由粗到细的随机搜索**、**贝叶斯优化**。

由粗到细的随机搜索可以缩小超高性能参数的范围，缺点是由一些手动的操作。贝叶斯优化是优化超参数最有效一种无需手动干涉的方式，具体操作请参考：

[https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f](https://link.zhihu.com/?target=https%3A//towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)

