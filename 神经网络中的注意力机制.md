人类的注意力机制（Attention Mechanism）是从直觉中得到，它是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段。深度学习中的注意力机制借鉴了人类的注意力思维方式，被广泛的应用在自然语言处理（Nature Language Processing，NLP）**[1]**、图像分类及语音识别等各种不同类型的深度学习任务中，并取得了显著的成果。因此，了解注意力机制的原理是深度学习中最重要的技术之一。

**【推荐阅读】**

注意力机制在NLP中应用的综述文章：

> Hu D. An Introductory Survey on Attention Mechanisms in NLP Problems[J]. arXiv preprint arXiv:1811.05544, 2018.

## **1. Attention机制的研究进展**

`Attention`机制最早是应用于图像领域，九几年就提出来的思想。在2014年，Google Mind团队发表的《Recurrent Models of Visual Attention》**[2]**论文使Attention机制开始火了起来，该论文提出在RNN模型上使用Attention机制来进行图像分类，结果取得了很好的性能。随后，在Bahdanau等人发表论文《Neural Machine Translation by Jointly Learning to Align and Translate》**[8]**中提出在机器翻译任务上使用Attention机制将翻译和对齐同时进行，他们的工作是第一个将Attention机制应用在NLP领域中的。接着，在Xu等人发表的论文《Show, Attend and Tell: Neural Image Caption Generation with Visual Attention》**[10]**中，成功的将Attention机制应用在Image Caption领域。从此，Attention机制就被广泛应用在基于RNN神经网络模型的各种深度学习任务中。随后，如何在CNN中使用Attention机制也成为研究的热点。2017年，Google发表的论文《Attention is all you need》**[12]**中提出在机器翻译上大量使用自注意力（self-attention）机制来学习文本表示。**图1**展示了Attention机制研究进展的大概趋势。

![img](https://pic1.zhimg.com/80/v2-d78af56f528565e0d1cbd3cb6043123c_720w.jpg)

图1：Attention机制研究进展的大概趋势



## **2. 人类的视觉注意力**

深度学习中的注意力机制借鉴了人类的注意力思维方式。因此，我们首先简单介绍人类视觉的选择性注意力。

视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是所说的注意力焦点，然后对这一区域投入更多的注意力资源，以获取更多所需要关注目标的细节信息，从而抑制其它无用信息。这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制。人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。

![img](https://pic2.zhimg.com/80/v2-a71e0fc80f4fa4ff4f0a9f2cd0c9e1bd_720w.jpg)图2：人类的视觉注意力

**图2**形象的展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标。很明显对于**图2**所示的场景，人们会把注意力更多的投入到人的脸部，文本的标题以及文章首句等位置。

深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。



## **3. 在图像中使用注意力机制**

由于基于注意力机制的神经网络在自然语言处理中广泛使用，并且取得了巨大的成功。因此，本文在介绍深度学习中的注意力机制的原理及关键计算机制时，会重点以Attention机制在NLP中的应用及发展为背景来详细介绍。但是，我们还是要简单介绍一下在图像领域中使用Attention的思想。

论文《Recurrent Models of Visual Attention》**[2]**是Attention机制在图像应用中的代表性文章，该论文提出人类看东西时并非将目光放在整张图片上，大多是根据需求将注意力集中到图像的特定部分。因此，提出了在传统的RNN中加入Attention机制，通过Attention去学习图像要处理的部分。该RAM模型能够顺序处理输入，在一个时刻根据一张图片内部的不同位置，把这些定位下的信息结合起来生成这个场景的动态内部表示。在每一步，该模型选择下一个位置，基于过去的信息和任务的需求。RAM的优势在于更少的像素需要处理，减少了任务的复杂度。

## **4. 在NLP中使用注意力机制**

### **4.1 Encoder-Decoder框架**

要了解深度学习中的注意力模型，就必须要知道`Encoder-Decoder`框架，因为目前大多数注意力机制都附着在Encoder-Decoder框架下。当然，我们也应该明白注意力机制是一种思想，本身并不依赖于任何框架。

Encoder-Decoder是深度学习中非常常见的一个模型框架。例如：在Image Caption的应用中Encoder-Decoder就是CNN-RNN的编码-解码框架；在神经网络机器翻译模型中Encoder-Decoder往往就是LSTM-LSTM的编码-解码框架。特别需要注意的是，在机器翻译中是文本到文本的转换，比如将法语翻译成英语，这样的Encoder-Decoder模型也被叫做Sequence to Sequence learning**[6]**。所谓编码，就是将输入序列编码成一个固定长度的向量；解码，就是将之前生成的固定向量再解码成输出序列。

![img](https://pic2.zhimg.com/80/v2-a4a6e4fad541c115d780a8c1e2384069_720w.jpg)图3：NLP中的Encoder-Decoder框架

在**图3**中为了方便阐述，我们选取Encoder和Decoder都是RNN。在RNN中，当前时刻隐藏层状态 ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bt%7D) 是由上一时刻的隐藏层状态 ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bt-1%7D) 和当前时刻的输入 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bt%7D)决定的，如**公式（1)**所示。

![[公式]](https://www.zhihu.com/equation?tex=h_%7Bt%7D%3Df%28h_%7Bt-1%7D%2Cx_%7Bt%7D%29) (1)

**在编码阶段**，获得了各个时刻的隐藏层状态后，我们把这些隐藏层的状态进行汇总，生成最后的语义编码向量C，如**公式（2）**所示，其中q表示某种非线性神经网络，在这里表示多层RNN。

![[公式]](https://www.zhihu.com/equation?tex=C%3Dq%28h_%7B1%7D%2Ch_%7B2%7D%2Ch_%7B3%7D%2C...%2Ch_%7BT_%7Bx%7D%7D%29) (2)

一种简单的方法是将最后的隐藏层状态作为语义编码向量C，即**公式（3）**所示。

![[公式]](https://www.zhihu.com/equation?tex=C%3Dq%28h_%7B1%7D%2Ch_%7B2%7D%2Ch_%7B3%7D%2C...%2Ch_%7BT_%7Bx%7D%7D%29%3Dh_%7BT_%7Bx%7D%7D) (3)

**在解码阶段**，我们要根据给定的语义向量C和之前已经生成的输出序列 ![[公式]](https://www.zhihu.com/equation?tex=y_%7B1%7D%2Cy_%7B2%7D%2Cy_%7B3%7D%2C...%2Cy_%7Bt-1%7D) 来预测下一个输出的单词 ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bt%7D) ,即公式（4）所示。

![[公式]](https://www.zhihu.com/equation?tex=y_%7Bt%7D%3DargmaxP%28y_%7Bt%7D%29%3D%5Cprod_%7Bt%3D1%7D%5E%7BT%7Dp%28y_%7Bt%7D%7C%7By_%7B1%7D%2Cy_%7B2%7D%2C...%2Cy_%7Bt-1%7D%7D%2CC%29) (4)

**公式（4）**可以简写成**公式（5）**。

![[公式]](https://www.zhihu.com/equation?tex=y_%7Bt%7D+%3D+g%28%7By_%7B1%7D%2Cy_%7B2%7D%2C...%2Cy_%7Bt-1%7D%7D%2CC%29) (5)

而在RNN中，**公式（5）**可以表示为公式（6）。

![[公式]](https://www.zhihu.com/equation?tex=y_%7Bt%7D%3Dg%28y_%7Bt-1%7D%2Cs_%7Bt-1%7D%2CC%29) (6)

**公式（6）**中， ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt-1%7D) 表示Decoder中RNN神经元的隐藏层状态， ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bt-1%7D) 表示前一时刻的输出，C代表的是语义向量。而g则是一个非线性的多层神经网络，可以输出 ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bt%7D) 的概率。g一般情况下是多层RNN后接softmax层。

**Encoder-Decoder框架虽然非常经典，但是局限性也非常大。**最大的局限性就在于编码器和解码器之间的唯一联系就是一个固定长度的语义向量C。**也就是说，编码器要将整个序列的信息压缩进一个固定长度的向量中。这样做存在两个弊端，一是语义向量C可能无法完全表示整个序列的信息，二是先输入到网络的内容携带的信息会被后输入的信息覆盖掉，输入序列越长，这个现象就越严重。**这两个弊端使得在解码的时候解码器一开始就没有获得输入序列足够多的信息， 那么解码的准确度自然也就不高了。

### **4.2 Attention机制**

为了解决Encoder-Decoder框架中的两个弊端，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》**[8]**中提出使用`Attention`机制。在深度学习领域，该论文是非常有影响力且具有开创性的，文中提出的Attention机制不仅应用于机器翻译中，还被推广到了其他应用领域。因此，该论文提出的`Attention`机制是非常值得深入学习。

![img](https://pic3.zhimg.com/80/v2-d4f78483d954433ee6a07cd75b2fbc86_720w.jpg)



图4：Seq-to-Seq with Attention（NMT)

#### **4.2.1 编码器**

图4展示了Bahdanau等人提出的机器翻译模型，在该模型中编码器就比较普通了，只是用了**双向循环神经网络**。在前向RNN中，数据是按顺序输入的，因此第j个隐藏层状态 ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bj%7D%5E%7B%5Crightarrow%7D) 只能携带第j个单词本身以及之前的一些信息。在逆向RNN中，数据是逆序输入的，则 ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bj%7D%5E%7B%5Cleftarrow%7D) 包含第j个单词及之后的信息。如果把这两个隐藏层状态结合起来， ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bj%7D%3D%5Cleft%5B+h_%7Bj%7D%5E%7B%5Crightarrow%7D%2Ch_%7Bj%7D%5E%7B%5Cleftarrow%7D+%5Cright%5D) 就包含了第j个输入的前后信息。

#### **4.2.2 解码器**

**图4**中展示的机器翻译模型中，编码过程相当简单，我们主要看一下解码的过程。

**第一步，**计算各个编码器隐藏层状态 ![[公式]](https://www.zhihu.com/equation?tex=h_%7B1%7D%5Csim+h_%7BT%7D) 与解码器隐藏层状态![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt-1%7D) 之间的相关程度，并进行`softmax`归一化操作得到每个隐藏层向量的权重 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bij%7D) ,计算公式如下：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D+e_%7Bij%7D%26%3Da%28s_%7Bi-1%7D%2Ch_%7Bj%7D%29+%5C%5C+%26%3Dv_%7Ba%7D%5E%7BT%7Dtanh%28W_%7Ba%7Ds_%7Bi-1%7D%2BU_%7Ba%7Dh_%7Bj%7D%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D)

![[公式]](https://www.zhihu.com/equation?tex=a_%7Bij%7D%3D%5Cfrac%7Bexp%28e_%7Bij%7D%29%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BT%7D%7Bexp%28e_%7Bik%7D%29%7D%7D)

![[公式]](https://www.zhihu.com/equation?tex=e_%7Bij%7D) 就表示第i个输出前一个隐藏层状态 ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bi-1%7D) 与第j个输入隐层向量 ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bj%7D) 之间的相关性，可以通过一个MLP神经网络进行计算，在图4中没有显示出来。得到 ![[公式]](https://www.zhihu.com/equation?tex=e_%7Bij%7D) 之后，将其传入到`softmax`函数就可以获得归一化的权重值 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bij%7D) 。

**第二步，**对 ![[公式]](https://www.zhihu.com/equation?tex=h_%7B1%7D%5Csim+h_%7BT%7D) 进行加权求和得到此次解码所对应的输入序列（source） ![[公式]](https://www.zhihu.com/equation?tex=%28x_%7B1%7D%2Cx_%7B2%7D%2C...%2Cx_%7BT%7D%29) 的编码向量 ![[公式]](https://www.zhihu.com/equation?tex=c_%7Bi%7D) ，计算公式如下：

![[公式]](https://www.zhihu.com/equation?tex=c_%7Bi%7D%3D%5Csum_%7Bj%3D1%7D%5E%7BT%7D%7Ba_%7Bij%7Dh_%7Bj%7D%7D)

**第三步，**有了 ![[公式]](https://www.zhihu.com/equation?tex=c_%7Bi%7D) 之后，我们根据编码向量 ![[公式]](https://www.zhihu.com/equation?tex=c_%7Bi%7D) 进行解码即可，先计算解码器 ![[公式]](https://www.zhihu.com/equation?tex=i) 时刻的隐藏层状态 ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bi%7D)，再计算解码器 ![[公式]](https://www.zhihu.com/equation?tex=i) 时刻的输出![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D) ，计算公式如下：

![[公式]](https://www.zhihu.com/equation?tex=s_%7Bi%7D+%3D+f%28s_%7Bi-1%7D%2Cy_%7Bi-1%7D%2Cc_%7Bi%7D%29)

![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D%3Dg%28y_%7Bi-1%7D%2Cs_%7Bi%7D%2Cc_%7Bi%7D%29)

这里的`Attention`机制采用的是Soft Attention，也就是对所有的编码器隐藏层状态 ![[公式]](https://www.zhihu.com/equation?tex=h_%7BT%7D) 向量都分配权重，可以将其理解为对其模型，作用是匹配输出序列（target)与source之间的对应关系。

#### **4.2.3 小结**

Attention机制最重要的步骤是如何在每一时刻产生不同的语言编码向量 ![[公式]](https://www.zhihu.com/equation?tex=c_%7Bi%7D) ，表示接下来输出的时候要重点关注输入序列中的哪些部分，然后根据关注的区域来产生下一个输出。模型可以形象化的表示为图5所示。

![img](https://pic1.zhimg.com/80/v2-2062157d1784fa7a35a483e613fce048_720w.jpg)图5：Encoder-Decoder+Attention机制

**相比于原始的Encoder-Decoder模型，加入Attention机制后最大的区别就是它不在要求编码器将所有输入信息都编码进一个固定长度的向量之中。而是，编码器需要将输入编码成一个向量的序列，在解码的时候，每一步都会选择性的从向量序列中挑选一个子集进行进一步处理。**这样，在产生每一个输出的时候，都能够做到充分利用输入序列携带的信息。而且这种方法在翻译任务中取得了非常不错的成果。

### **4.3 Global Attention 和 Local Attention**

继上一篇论文之后，Luong等人在Attention机制上也发表了一篇很有代表性的文章《 Effective approaches to attention-based neural machine translation》**[9]**，该论文提出了两种Attention机制，一种是`Global Attention`，另一种是`Local Attention`。该论文对后续各种基于Attention机制的模型在NLP中的应用起到了很大的促进作用。

#### **4.3.1 Global Attention**

`Global Attention`其实和论文《Neural Machine Translation by Jointly Learning to Align and Translate》提出的Attention机制思路是一样的，它也是对输入序列中所有词进行处理，只是在计算权值 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bij%7D) （即 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bt%7D%28s%29) ）的时候稍有差别，在计算![[公式]](https://www.zhihu.com/equation?tex=h_%7B1%7D%5Csim+h_%7BT%7D) 各个隐藏层向量与 ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt-1%7D) 之间的相关程度由神经网络变成了下面三种选择，包含`内积`、`general`、`concat`，并且实验结果表明`general`效果比较好。

![img](https://pic3.zhimg.com/80/v2-5f98955160cdab66121991ab2cd7f06e_720w.jpg)图6：Global Attention Model

图6中score函数就是计算解码器 ![[公式]](https://www.zhihu.com/equation?tex=t) 时刻隐藏层状态 ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bt%7D) 与编码器所有隐藏层状态 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbar%7Bh_%7Bs%7D%7D) 之间的相似度。

**注意：**这里的 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bt%7D) 和上面提到的 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bij%7D) 都表示当前时刻target中某个单词和source中每个单词对应的注意力分配概率。只是不同的论文，使用不同的符号而已。

`Global Attention`也属于Soft Attention。**Soft Attention的缺点是每次Decoder的时候都要计算所有的编码器隐藏层状态向量，导致计算复杂度较高，而且很容易可以想到，其实有些source跟本次Decoder根本没有任何关系，所以计算它们之间的相似度有些多余。除此之外，当source序列较长时，Soft Attention这种方法的效果也会有所下降。**而Hard Attention每次仅选择一个相关的source进行计算，这种缺点是不可微，没有办法进行反向传播，只能借助强化学习等手段进行训练。关于Hard Attention的内容可以参考论文《Show, Attend and Tell: Neural Image Caption Generation with Visual Attention》**[10]**。为了融合Soft Attention和Hard Attention，Luong等人又提出了一种Local Attention机制。

#### **4.3.2 Local Attention**

`Local Attention`机制每次只选择一部分source进行计算，这样既可以减少计算量、又可微，效果也更好。**Local Attention的思路是：首先会为Decoder端当前的词预测一个source端对其位置（Aligned Position）pt，然后基于pt为中心选择一个窗口用于计算语言编码向量![[公式]](https://www.zhihu.com/equation?tex=c_%7Bi%7D)。这个过程中最重要的就是如何确定中心pt的位置**，论文中提出了两种方法：

- **Monotonic alignment：**直接选择source序列中的第t个作为中心pt，然后向两侧取窗口大小个词。
- **Predictive alignment：**使用图7中红框中的公式决定pt值的大小，将Decoder的隐状态 ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bt%7D) 经过一个激活函数tanh和sigmoid两个函数之后变成0~1，再乘以S（source序列长度）就变成0~S，也就是我们想要的原序列所对应的中心位置pt，取出窗口大小个隐层，在进行加权求和。这里权重乘以了一个高斯分布，目的是让靠近中心pt的词的权重更大一点，让 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bt%7D) 形成一种钟型分布，这里的 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bt%7D) 表示注意力分配概率。原本的 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bt%7D) 再乘以高斯分布之后值发生变化，导致靠近中心的 ![[公式]](https://www.zhihu.com/equation?tex=a_%7Bt%7D) 增加，两侧的减小。

![img](https://pic1.zhimg.com/80/v2-5f0f052f042ed0e63ead98488a27fe9c_720w.jpg)图7：Local Attention Model

## **5. 在CNN中使用Attention机制**

由于在Encoder-Decoder框架中加入Attention机制组成的RNN模型在NLP中广泛使用，不仅应用在序列到序列学习模型中，还应用在各种分类问题中。研究者们开始思考在深度学习中卷积神经网络CNN是否也可以使用Attention机制呢？Yin 等人在论文《ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs》[11]中提出了3种方法在CNN中使用Attention机制，这是Attention在CNN中较早的探索性工作。

- 第一种方法ABCNN0-1是在卷积前进行Attention。
- 第二种方法ABCNN-2是在池化时进行Attention。
- 第三种就是把前两种方法一起用到CNN中。

Yin等人的论文提供了我们在CNN中使用Attention机制的思路。现在也有不少使用基于Attention的CNN工作，并取得了不错的效果。由于在CNN中使用Attention机制并不是我们这篇文章的重点，我在这里只是做了一个抛砖引玉。如果对本部分感兴趣的同学，可以自己查阅相关文献。

## **6. Self Attention机制**

通过第4节对Attention机制原理的讲解以及对Attention机制的扩展做了详细的介绍，我们可以很容易的理解本节所介绍的Self Attention机制。Vaswani等人在论文《Attention is all you need》[12]中提出不使用RNN或CNN等复杂的模型，仅仅依赖于Attention模型可以使训练并行化且拥有全局信息，该模型在机器翻译领域不同规模的公开数据集上都表现良好。并且，Google最新的机器翻译模型内部大量采用了Self Attention机制。

普通的Attention机制都发生在输出Target句子中某个单词和输入Source句子每个单词之间的相似度。而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制。

![img](https://pic1.zhimg.com/80/v2-cce0708becb5f99f2e471693b41cc2b8_720w.jpg)图8：可视化Self Attention实例

在**图8**中我们展示了可视化后的Self Attention机制，Self Attention可以捕获同一个句子中单词之间的一些句法特征或者语义特征。图8展示了Self Attention捕获了同一个句子中单词之间的语义特征，“it”的指代对象是“the animal”。

很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或LSTM需要根据时间序列计算Target，对于远距离的相互依赖的特征，要经过若干时间步的信息累积才能将两者联系起来，而距离越远，捕获有效信息的可能性越小。但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算结果直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。并且，Self Attention对于增加计算的并行性也有直接帮助作用。所以，这也是Self Attention逐渐被广泛使用的主要原因。

## **7. Reference**

【1】Hu D. An Introductory Survey on Attention Mechanisms in NLP Problems[J]. arXiv preprint arXiv:1811.05544, 2018.

【2】Mnih V, Heess N, Graves A. Recurrent models of visual attention[C]//Advances in neural information processing systems. 2014: 2204-2212.

【3】[注意力机制（Attention Mechanism）](https://link.zhihu.com/?target=https%3A//blog.csdn.net/yimingsilence/article/details/79208092)

【4】[知无我：浅谈Attention机制的理解](https://zhuanlan.zhihu.com/p/35571412)

【5】[深度学习中的注意力机制](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA4Mzc0NjkwNA%3D%3D%26mid%3D2650783542%26idx%3D1%26sn%3D3846652d54d48e315e31b59507e34e9e%26chksm%3D87fad601b08d5f17f41b27bb21829ed2c2e511cf2049ba6f5c7244c6e4e1bd7144715faa8f67%26mpshare%3D1%26scene%3D1%26srcid%3D1113JZIMxK3XhM9ViyBbYR76%23rd)

【6】Sutskever I, Vinyals O, Le Q V. Sequence to sequence learning with neural networks[C]//Advances in neural information processing systems. 2014: 3104-3112.

【7】[深度学习笔记(六)：Encoder-Decoder模型和Attention模型 - Multiangle's Notepad - CSDN博客](https://link.zhihu.com/?target=https%3A//blog.csdn.net/u014595019/article/details/52826423)

【8】Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.

【9】Luong M T, Pham H, Manning C D. Effective approaches to attention-based neural machine translation[J]. arXiv preprint arXiv:1508.04025, 2015.

【10】Xu K, Ba J, Kiros R, et al. Show, attend and tell: Neural image caption generation with visual attention[C]//International conference on machine learning. 2015: 2048-2057.

【11】Yin W, Schütze H, Xiang B, et al. Abcnn: Attention-based convolutional neural network for modeling sentence pairs[J]. arXiv preprint arXiv:1512.05193, 2015.

【12】Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in Neural Information Processing Systems. 2017: 5998-6008.

【13】[lqfarmer：模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用](https://zhuanlan.zhihu.com/p/31547842)